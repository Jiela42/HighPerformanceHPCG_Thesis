AMGX,GH200,128x128x128,SymGS
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","61'954",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.14",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.14",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","40.61",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.37",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.94",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","55'354.14",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.86",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.37",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.27",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.50",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.38",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.50",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.08"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","35.13",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.14",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.59",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.75",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.76",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.87"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","25.19"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","35.04",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.35",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","64.96",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.77",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.72",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.77 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.86"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.31",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.49",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.19",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.57",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 29.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.2% of the total average of 39.3 cycles between issuing two instructions.","global","56.86"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.2 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14.36"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","19'007.24",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","10'035'822",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19'094.42",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","10'081'852",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 753255 fused and 585865 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.13"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.77",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.17",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.84"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","45'820.83",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","5'098'496",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","55'354.14",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'912'100",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","61'611.09",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'579'744",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","55'354.14",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'912'100",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","54'499.19",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","31'648'400",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.00% above the average, while the minimum instance value is 21.37% below the average.","global","5.545"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.00% above the average, while the minimum instance value is 21.37% below the average.","global","5.545"
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'214'738",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","317.03",
"0","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 582426 excessive sectors (21% of the total 2729435 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.18"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","61'894",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.02",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.02",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","40.54",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.36",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","54.12",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","55'398.26",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.70",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.37",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.26",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.45",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.38",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.45",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.1"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.95",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.02",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.53",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.28",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.65",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.95"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","25.1"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","34.75",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.35",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","65.25",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.98",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.72",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.98 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.98"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.24",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.42",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.19",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.57",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.0% of the total average of 40.2 cycles between issuing two instructions.","global","56.98"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.2 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14.29"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","18'999.47",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","10'031'718",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19'084.26",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","10'076'487",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 752931 fused and 585613 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.127"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.54",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.39",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.08"
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","45'649.67",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","5'093'376",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","55'398.26",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'945'780",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","62'754.02",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'576'576",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","55'398.26",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'945'780",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","54'924.71",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","31'783'120",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'214'234",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","316.89",
"1","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 581548 excessive sectors (21% of the total 2727686 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.53"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","57'792",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.54",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.54",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","37.89",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.10",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","54.67",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","51'213.93",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.21",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.37",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.24",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.32",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.37",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.32",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.18"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.72",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.54",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.51",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.06",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.22",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","20.15"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.62"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","34.36",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","65.64",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.77",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.72",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.77 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.46"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.07",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.30",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.27",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.64",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.9% of the total average of 40.1 cycles between issuing two instructions.","global","56.46"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.3 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17'479.90",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","9'229'386",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","17'577.49",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","9'280'914",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 689589 fused and 536347 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.107"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.45",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.33",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.18"
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","43'173.17",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'759'808",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","51'213.93",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'434'384",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","56'689.28",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'144'960",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","51'213.93",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'434'384",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","51'155.94",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","29'737'536",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'115'702",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","290.23",
"2","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 536782 excessive sectors (21% of the total 2506653 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.97"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","57'786",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.55",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.55",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","37.86",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.12",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","54.64",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","51'119.45",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.25",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.37",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.24",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.33",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.37",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.33",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.16"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.74",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.55",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.49",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.15",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.26",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","20.12"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.66"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","34.34",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","65.66",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.13",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.13 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.45"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.13",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.33",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.27",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.64",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.6% of the total average of 41.1 cycles between issuing two instructions.","global","56.45"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.3 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14.02"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17'464.78",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","9'221'406",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","17'549.80",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","9'266'295",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 688959 fused and 535857 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.109"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.17",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.15",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.46"
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","43'159.17",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'757'248",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","51'119.45",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'413'382",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","56'965.96",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'143'712",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","51'119.45",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'413'382",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","51'104.87",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","29'653'528",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'114'722",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","289.97",
"3","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 533650 excessive sectors (21% of the total 2501226 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.99"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","54'901",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.55",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.55",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","36",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.98",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.77",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","47'654.70",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.49",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.21",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.31",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.31",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.67"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.71",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.07",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.55",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.31",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.14",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.65",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.48"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.01"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.92",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.08",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.80",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.80 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.45"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.69",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.91",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.36",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.72",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.8% of the total average of 40.7 cycles between issuing two instructions.","global","57.45"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.61"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'788.03",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'336'082",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'872.07",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'380'455",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 619065 fused and 481495 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.033"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.57",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.76",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.07"
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","40'062.33",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'519'040",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","47'654.70",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'872'000",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","53'052.82",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'833'920",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","47'654.70",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'872'000",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","46'789.37",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","27'488'000",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'005'998",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","260.55",
"4","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 481171 excessive sectors (21% of the total 2250126 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.67"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","55'471",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.21",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.21",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","36.35",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","35.13",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.07",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","47'662.42",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.19",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.20",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.44",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.44",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.6"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.70",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.79",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.21",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.36",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.72",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.44",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.22"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.79"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.51",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.49",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.53",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.53 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.79"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.39",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.61",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.35",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.71",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.9% of the total average of 40.4 cycles between issuing two instructions.","global","57.79"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.48"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'852.16",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'369'940",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'937.77",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'415'145",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 621738 fused and 483574 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.042"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.52",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.73",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.12"
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","40'183.83",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'569'088",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","47'662.42",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'967'364",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","52'936.04",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'899'296",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","47'662.42",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'967'364",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","47'564.04",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","27'869'456",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'010'156",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","261.67",
"5","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 482859 excessive sectors (21% of the total 2259071 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.41"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","51'959",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.64",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.64",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","34.05",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.82",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.53",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","47'253.02",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.85",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.29",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.31",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.31",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.15"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.32",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.64",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.26",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.27",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.17",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.76"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.43"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.03",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.97",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.37",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.37 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.36"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.46",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.68",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.40",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.75",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.0% of the total average of 40.5 cycles between issuing two instructions.","global","56.36"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.3"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'186.51",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'018'478",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'269.55",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'062'324",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 593991 fused and 461993 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.967"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.05",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.43",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.6"
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","38'890.83",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'277'248",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","47'253.02",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'751'264",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","51'301.72",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'519'040",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","47'253.02",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'751'264",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","46'222.65",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","27'005'056",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","966'994",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","250.00",
"6","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 460063 excessive sectors (21% of the total 2157722 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.03"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","53'519",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.36",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.36",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","35.10",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.84",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.44",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","45'829.58",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.86",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.28",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.28",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.68"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.70",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.49",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.36",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.32",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.27",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.17",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.38"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.44"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.83",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.17",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.50",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.50 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.64"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.13",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.36",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.40",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.75",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.9% of the total average of 41.1 cycles between issuing two instructions.","global","57.64"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.3"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'167.51",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'008'446",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'252.80",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'053'481",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 593199 fused and 461377 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.026"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.96",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.37",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.7"
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","38'879.83",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'406'016",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","45'829.58",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'742'670",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","49'966.45",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'687'136",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","45'829.58",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'742'670",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","46'461.25",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'970'680",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","965'762",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","249.66",
"7","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 463450 excessive sectors (21% of the total 2158923 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.11"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","50'948",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.79",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.79",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","33.41",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.04",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.02",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","44'439.83",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.37",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.30",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.63",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.63",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.72",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.40",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.79",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.15",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.04",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.78",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.23"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.98"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.71",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.29",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.47",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.47 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.21"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.18",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.42",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.45",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.79",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 81.4% of the total average of 41.2 cycles between issuing two instructions.","global","57.21"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.04"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14'417.88",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'612'638",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'501.57",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'656'828",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 561951 fused and 437073 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.979"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.08",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.09",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.55"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","37'415.33",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'196'608",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","44'439.83",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'518'526",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","50'278.78",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'410'656",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","44'439.83",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'518'526",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","44'328.70",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'074'104",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.12% above the average, while the minimum instance value is 21.37% below the average.","global","5.509"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.12% above the average, while the minimum instance value is 21.37% below the average.","global","5.509"
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","917'154",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","236.51",
"8","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 436883 excessive sectors (21% of the total 2043639 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.07"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","50'473",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.09",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.09",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","33.12",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.18",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.80",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","44'147.61",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.19",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.30",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.74",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.74",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.95"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.61",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.09",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.15",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.98",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.65",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.15"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.86"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.07",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.93",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.70",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.70 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.91"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.42",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.66",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.45",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.79",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.3% of the total average of 41.4 cycles between issuing two instructions.","global","56.91"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.96"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14'369.08",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'586'874",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'454.03",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'631'727",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 559917 fused and 435491 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.985"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.44",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.68",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.2"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","37'339.50",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'159'744",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","44'147.61",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'535'138",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","48'780.76",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'361'024",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","44'147.61",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'535'138",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","43'709.43",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'140'552",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.92% above the average, while the minimum instance value is 23.22% below the average.","global","5.227"
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","913'990",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","235.66",
"9","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 435618 excessive sectors (21% of the total 2036591 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.68"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","50'637",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.98",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.98",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","33.25",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.63",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","50.46",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","42'408.68",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.35",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.13",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.22",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.22",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.71"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.69",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.64",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.98",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.27",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.63",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.03",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.63"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.17"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.44",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.56",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.73",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.73 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.02"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.31",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.57",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.48",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.82",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.7% of the total average of 42.3 cycles between issuing two instructions.","global","58.02"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.57"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14'002.90",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'393'530",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'088.87",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'438'922",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 544653 fused and 423619 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.01"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.17",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.79",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.45"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","36'522.50",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'175'872",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","42'408.68",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'559'694",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","47'760.43",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'380'896",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","42'408.68",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'559'694",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","43'428.73",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'238'776",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.41% above the average, while the minimum instance value is 19.31% below the average.","global","5.466"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.45% above the average, while the minimum instance value is 19.51% below the average.","global","6.51"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.41% above the average, while the minimum instance value is 19.31% below the average.","global","5.466"
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","890'246",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","229.23",
"10","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 423857 excessive sectors (21% of the total 1981534 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.23"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","49'669",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.79",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.79",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","32.58",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.90",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.11",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","43'224.86",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.12",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.29",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.57",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.30",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.57",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.05"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.72",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.23",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.79",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.23",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.58",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.56",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.86"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.74"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.48",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.52",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.29",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.72",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.29 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.21"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.67",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.97",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.48",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.82",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.8% of the total average of 42.7 cycles between issuing two instructions.","global","57.21"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.91"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'980.23",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'381'560",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'077.09",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'432'701",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 543708 fused and 422884 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.969"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.59",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.42",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.03"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","36'477.67",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'091'904",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","43'224.86",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'380'916",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","49'404.92",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'274'432",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","43'224.86",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'380'916",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","42'048.62",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","25'523'664",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.58% above the average, while the minimum instance value is 18.52% below the average.","global","6.595"
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","888'776",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","228.83",
"11","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 421593 excessive sectors (21% of the total 1976630 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.18"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'522",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.13",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.13",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.84",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.73",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.27",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'577.51",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.35",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.21",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.36",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.36",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.66"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.19",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.13",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.54",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.05",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.41",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.87"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.7"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.28",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.72",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.15",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.15 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.87"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.53",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.83",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.49",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.83",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.6% of the total average of 42.5 cycles between issuing two instructions.","global","56.87"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.44"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'771.66",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'271'436",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'868.96",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'322'811",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 535014 fused and 416122 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.014"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","88.12",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.39",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (88.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.49"
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'930.83",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'998'720",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'577.51",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'032'300",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","46'157.36",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'149'248",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'577.51",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'032'300",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'675.32",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'129'200",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","875'252",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","225.17",
"12","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 416286 excessive sectors (21% of the total 1946409 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.4"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'503",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.09",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.09",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.84",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.62",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.55",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'569.78",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.85",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.25",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.25",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.7"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.21",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.09",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.55",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.69",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.07",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.98"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.31"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.17",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.83",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.83",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.83 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.91"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.68",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.94",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.49",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.83",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 78.6% of the total average of 41.7 cycles between issuing two instructions.","global","56.91"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.22"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'735.82",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'252'512",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'820.19",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'297'062",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 533520 fused and 414960 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.009"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.68",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.48",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.94"
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'898.17",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'998'464",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'569.78",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'111'212",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'509.55",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'148'672",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'569.78",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'111'212",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'661.00",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'444'848",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","872'928",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","224.55",
"13","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 415480 excessive sectors (21% of the total 1941115 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.16"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'598",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.54",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.54",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.30",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.14",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.07",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'889.30",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.70",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.30",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.18",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.82",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.82",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.91"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.52",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.54",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.77",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.03",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.96",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.1"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.18"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.19",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.81",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.17",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.17 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.46"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.70",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.97",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.50",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.3% of the total average of 42.7 cycles between issuing two instructions.","global","56.46"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.15"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'660.90",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'212'954",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'748.62",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'259'271",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 530397 fused and 412531 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.982"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.69",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.12",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.92"
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'610.50",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'926'016",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'889.30",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'109'820",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'688.73",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'055'072",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'889.30",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'109'820",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'421.09",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'439'280",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","868'070",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","223.23",
"14","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 411565 excessive sectors (21% of the total 1928348 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.52"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'160",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.78",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.78",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.98",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.36",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.37",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'353.87",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.94",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.02",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.02",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.81"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.76",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.50",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.78",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.74",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.42",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.12",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.24"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.37"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.09",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.91",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.74",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.74 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.22"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.53",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.79",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.6% of the total average of 41.5 cycles between issuing two instructions.","global","56.22"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.25"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'568.49",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'164'162",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'653.91",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'209'263",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 526545 fused and 409535 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.993"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.19",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.80",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.43"
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'475.67",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'889'152",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'353.87",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'020'190",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'884.18",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'004'672",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'353.87",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'020'190",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'258.80",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'080'760",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","862'078",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.61",
"15","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 411210 excessive sectors (21% of the total 1917045 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.88"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'114",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.00",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.00",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.94",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.44",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.60",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'459.12",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.48",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.08",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.08",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.78"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.72",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.00",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.77",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.88",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.80",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.31"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.02"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.11",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.89",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.41",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.41 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.75",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.01",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.50",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.9% of the total average of 41.8 cycles between issuing two instructions.","global","56"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.05"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'630.67",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'196'994",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'715.62",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'241'850",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 529137 fused and 411551 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.998"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.82",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.20",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.79"
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'606",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'884'288",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'459.12",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'141'686",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'139.76",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'002'560",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'459.12",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'141'686",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","42'709.25",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'566'744",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","866'110",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","222.70",
"16","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 411653 excessive sectors (21% of the total 1924903 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.53"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'637",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.30",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.30",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.30",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.74",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.78",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'875.40",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.97",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.41",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.41",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.64"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.14",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.30",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.67",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.95",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.12",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.02"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.37"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.62",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.38",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.08",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.08 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.7"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.89",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.21",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.4% of the total average of 41.9 cycles between issuing two instructions.","global","56.7"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.26"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'554.45",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'156'752",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'658.04",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'211'445",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 525960 fused and 409080 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.014"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.22",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.82",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.39"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'417.83",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'926'528",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'875.40",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'015'462",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'602.59",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'060'160",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'875.40",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'015'462",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'625.61",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'061'848",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.61% above the average, while the minimum instance value is 17.24% below the average.","global","5.036"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.61% above the average, while the minimum instance value is 17.24% below the average.","global","5.036"
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","861'168",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.36",
"17","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 410016 excessive sectors (21% of the total 1914596 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.53"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'086",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.10",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.10",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.55",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.92",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.23",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'741.49",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.51",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.54",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.54",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.56"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.08",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.10",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.42",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.48",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.82",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.24"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.05"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.90",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.10",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.76",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.76 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.9"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.82",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.08",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.9% of the total average of 41.8 cycles between issuing two instructions.","global","56.9"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.05"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'579.72",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'170'090",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'664.91",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'215'070",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 527013 fused and 409899 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.025"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.76",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.52",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.87"
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'584.17",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'963'392",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'741.49",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'111'868",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'061.48",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'107'008",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'741.49",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'111'868",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'540.17",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'447'472",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","862'806",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.81",
"18","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 410801 excessive sectors (21% of the total 1918843 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.13"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'739",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.38",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.38",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.36",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.26",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.13",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'455.67",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.80",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.18",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.92",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.92",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.86"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.29",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.38",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.42",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.34",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.02",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.22"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.26"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.33",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.67",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.99",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.99 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.62"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.96",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.22",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.2% of the total average of 42.0 cycles between issuing two instructions.","global","56.62"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.18"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'560.93",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'160'172",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'646.10",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'205'143",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 526230 fused and 409290 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.987"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.23",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.83",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.39"
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'585.33",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'937'280",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'455.67",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'044'644",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'548.40",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'071'008",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'455.67",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'044'644",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'936.78",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'178'576",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","861'588",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.48",
"19","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 410261 excessive sectors (21% of the total 1915666 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.47"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'807",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.12",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.12",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.72",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.67",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.96",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'918.97",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.92",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.29",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.35",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.35",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.14"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.87",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.12",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.19",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.20",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.10",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.6"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.36"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.12",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.88",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.70",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.70 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","55.88"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.35",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.62",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.52",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.85",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 79.8% of the total average of 41.4 cycles between issuing two instructions.","global","55.88"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.23"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'473.70",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'114'116",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'559.05",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'159'178",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 522594 fused and 406462 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.951"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.49",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.35",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.14"
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'473.17",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'859'712",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'918.97",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'981'914",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'689.86",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'969'152",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'918.97",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'981'914",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'936.46",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'927'656",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","855'932",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","219.95",
"20","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 408966 excessive sectors (21% of the total 1904917 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.54"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'130",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.71",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.71",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.94",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","35.06",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.32",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'152.02",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.87",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.34",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.67",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.35",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.67",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.49"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.76",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.50",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.71",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.18",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.96",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.06",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.34"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.33"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.33",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.67",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.94",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.94 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.29"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.83",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.09",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.53",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.86",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.4% of the total average of 41.8 cycles between issuing two instructions.","global","56.29"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.2"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'433.98",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'093'140",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'518.51",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'137'772",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 520938 fused and 405174 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.031"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.96",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.65",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.66"
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'404.17",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'887'872",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'152.02",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'974'680",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'339.27",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'005'152",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'152.02",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'974'680",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'556.23",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'898'720",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","853'356",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","219.25",
"21","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 406406 excessive sectors (21% of the total 1897593 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.21"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'511",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.17",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.17",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.59",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.90",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.09",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'133.61",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.49",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.30",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.58",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.30",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.58",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.03"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.85",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.17",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.99",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.55",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.79",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.67"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.01"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.17",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.83",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.17",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.17 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","55.83"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.73",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.00",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.54",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.87",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 78.0% of the total average of 42.7 cycles between issuing two instructions.","global","55.83"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.02"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'317.39",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'031'580",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'402.82",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'076'687",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 516078 fused and 401394 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.964"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.67",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.47",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.96"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'297.33",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'835'648",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'133.61",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'000'236",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'890.02",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'938'240",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'133.61",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'000'236",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'403.11",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'000'944",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.69% above the average, while the minimum instance value is 17.74% below the average.","global","5.054"
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","845'796",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","217.20",
"22","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 402852 excessive sectors (21% of the total 1880754 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.69"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'343",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.19",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.19",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.46",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.51",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.83",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'231.95",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.24",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.15",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.15",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.75"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.71",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.19",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.96",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.89",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.61",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.56"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.83"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.50",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.50",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.60",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.60 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","55.81"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.84",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.10",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.54",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.87",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.3% of the total average of 41.8 cycles between issuing two instructions.","global","55.81"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.91"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'252.61",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'997'380",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'336.95",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'041'911",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 513378 fused and 399294 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.997"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.55",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.39",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.08"
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'190.83",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'822'080",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'231.95",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'020'568",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'658.69",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'917'600",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'231.95",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'020'568",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'035.82",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'082'272",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","841'596",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","216.07",
"23","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 401012 excessive sectors (21% of the total 1871269 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.1"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'124",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.97",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.97",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.27",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.18",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.88",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'085.73",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.26",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.84",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.84",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.9"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.53",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.97",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.89",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.26",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.61",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.62"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.84"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.66",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.34",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.87",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.87 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.03"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.47",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.74",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.56",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.89",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.2% of the total average of 42.5 cycles between issuing two instructions.","global","56.03"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.6 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.9"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'081.61",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'907'092",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'165.78",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'951'533",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 506250 fused and 393750 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.977"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.05",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.07",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.59"
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34'832.67",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'802'624",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'085.73",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'938'846",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'167.96",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'889'088",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'085.73",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'938'846",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'315.17",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'755'384",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","830'508",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","213.07",
"24","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 397189 excessive sectors (21% of the total 1847999 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.06"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'429",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.70",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.70",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.17",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.46",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.20",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","39'495.50",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.10",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.18",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.18",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.75"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.72",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.39",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.70",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.88",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.98",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.47",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.98"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.66"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.61",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.39",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.93",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.93 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.3"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.71",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.03",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.57",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.90",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.6% of the total average of 42.7 cycles between issuing two instructions.","global","57.3"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.6 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.82"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'007.34",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'867'876",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'106.34",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'920'148",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 503154 fused and 391342 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.994"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.39",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.29",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.24"
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34'801",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'912'448",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","39'495.50",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'946'136",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'039.82",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'040'192",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","39'495.50",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'946'136",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'193.47",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'784'544",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","825'692",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","211.77",
"25","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 393541 excessive sectors (21% of the total 1835812 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.98"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","45'219",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.00",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.00",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","29.70",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.94",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.42",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'430.17",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.34",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.26",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.13",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.71",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.27",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.71",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.46"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.31",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.00",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.82",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.65",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","19.94",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.45"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.08"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.15",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.85",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.01",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.01 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.58",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.87",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.60",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.92",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.0% of the total average of 43.6 cycles between issuing two instructions.","global","56"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.6 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.47"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12'734.22",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'723'666",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12'818.98",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'768'423",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 491769 fused and 382487 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.904"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.20",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.17",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.43"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34'183.50",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'729'408",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'430.17",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'969'928",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","43'985.66",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'797'120",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'430.17",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'969'928",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","39'873.39",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'879'712",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 6.14% above the average, while the minimum instance value is 4.56% below the average.","global","5.418"
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","807'982",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","206.97",
"26","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 386063 excessive sectors (21% of the total 1796824 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.91"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","44'996",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.61",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.61",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","29.54",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.39",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.90",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","39'186.38",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.69",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.28",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.14",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.17",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.17",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.23"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.79",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.61",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.77",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.19",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.17",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.26"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.33"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.54",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.46",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.84",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.84 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.39"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.52",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.81",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.62",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.94",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.3% of the total average of 42.5 cycles between issuing two instructions.","global","56.39"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.6 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.6"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12'521.11",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'611'148",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12'606.39",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'656'173",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 482886 fused and 375578 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.929"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.06",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.08",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.58"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","33'714",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'710'464",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","39'186.38",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'799'996",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","42'761.71",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'775'904",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","39'186.38",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'799'996",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","38'745.66",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'199'984",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.04% above the average, while the minimum instance value is 5.13% below the average.","global","5.329"
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","794'164",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","203.23",
"27","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 379281 excessive sectors (21% of the total 1765435 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.47"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","44'436",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.85",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.85",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","29.18",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.92",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","49.85",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","38'215.77",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.80",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.26",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.14",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.77",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.27",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.77",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.43"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.72",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","30.88",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.85",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.85",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.33",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.21",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.49"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.38"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.51",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.49",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.91",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.91 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.15"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.80",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.10",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.67",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.98",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 78.2% of the total average of 42.8 cycles between issuing two instructions.","global","57.15"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.7 threads being active per cycle. This is further reduced to 18.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.62"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12'056.05",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'365'592",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12'140.55",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'410'211",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 463500 fused and 360500 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.898"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.04",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.07",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.59"
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","32'695",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'662'848",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","38'215.77",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'563'868",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","42'011.81",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'712'736",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","38'215.77",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'563'868",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","37'345.42",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","22'255'472",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","764'008",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","195.08",
"28","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 365456 excessive sectors (22% of the total 1696886 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.43"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","42'656",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.20",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.20",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","28.03",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.75",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","50.94",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","36'869.70",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","27.52",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.25",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.09",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.59",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.26",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.59",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.52"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","30.93",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.20",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.66",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.19",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","19.27",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.93"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","21.4"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.44",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.56",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.65",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.65 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.8"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.42",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.74",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.73",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.03",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.3% of the total average of 43.4 cycles between issuing two instructions.","global","56.8"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.7 threads being active per cycle. This is further reduced to 18.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.01"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11'562.48",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'104'988",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","11'647.68",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'149'974",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 442926 fused and 344498 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.88"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.23",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.55",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.41"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","31'661.17",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'517'568",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","36'869.70",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'586'764",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","41'449.39",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'526'304",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","36'869.70",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'586'764",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","37'042.92",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","22'347'056",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.13% above the average, while the minimum instance value is 19.67% below the average.","global","5.369"
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","732'004",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","186.42",
"29","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 350217 excessive sectors (22% of the total 1623541 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.96"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","41'820",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.90",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.90",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","27.39",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.17",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","48.96",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","36'264.01",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","27.07",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.07",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.17",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.21",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.17",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.21"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.68",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","29.27",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.90",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.75",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","22.21",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","18.89",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.15"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","20.98"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.11",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.89",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.48",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.48 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.1"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.35",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.68",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.81",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.10",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.5% of the total average of 43.3 cycles between issuing two instructions.","global","58.1"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.8 threads being active per cycle. This is further reduced to 18.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11.76"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","10'858.18",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","5'733'120",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10'942.39",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","5'777'584",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 413568 fused and 321664 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.785"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.53",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.46",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.14"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","30'026.33",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'439'616",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","36'264.01",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'335'264",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","40'279.47",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'423'968",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","36'264.01",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'335'264",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","35'178.54",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","21'341'056",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.60% above the average, while the minimum instance value is 7.34% below the average.","global","5.024"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.91% above the average, while the minimum instance value is 8.10% below the average.","global","6.01"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.60% above the average, while the minimum instance value is 7.34% below the average.","global","5.024"
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","686'336",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","174.06",
"30","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 326349 excessive sectors (22% of the total 1515981 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.82"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","38'986",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.87",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.87",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","25.60",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","30.18",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","47.86",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","33'999.07",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","25.78",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.17",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.51",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.51",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.55"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.68",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","28.72",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.87",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.37",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","21.07",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","17.90",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","17.86"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","19.77"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.85",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.15",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.77",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.77 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.13"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.13",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.54",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.93",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.20",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 34.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.8% of the total average of 46.1 cycles between issuing two instructions.","global","58.13"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.9 threads being active per cycle. This is further reduced to 18.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11.12"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9'946.40",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","5'251'698",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10'034.26",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","5'298'090",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 375561 fused and 292103 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.729"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.31",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.60",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.34"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","28'047.50",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'215'104",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","33'999.07",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'137'238",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","36'118.54",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'132'704",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","33'999.07",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'137'238",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","33'613.30",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","20'548'952",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.77% above the average, while the minimum instance value is 19.42% below the average.","global","5.041"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.59% above the average, while the minimum instance value is 20.17% below the average.","global","6.557"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.77% above the average, while the minimum instance value is 19.42% below the average.","global","5.041"
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","627'214",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","158.06",
"31","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 298863 excessive sectors (22% of the total 1380087 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.17"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","35'216",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.65",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.65",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","23.14",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","30.47",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","46.41",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","29'607.03",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","25.78",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.19",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.07",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.07",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.27"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.67",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","27.53",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.65",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.23",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","20.78",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","17.77",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","17.34"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","19.59"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.46",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.54",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.92",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.92 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.35"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","47.26",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.71",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","19.11",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.35",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 34.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 72.5% of the total average of 47.3 cycles between issuing two instructions.","global","58.35"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 19.1 threads being active per cycle. This is further reduced to 18.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8'819.35",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","4'656'618",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","8'903.01",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","4'700'788",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 328581 fused and 255563 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.737"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.62",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.43",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.01"
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","25'210.17",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'905'600",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","29'607.03",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","4'557'744",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","32'054.36",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","3'725'760",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","29'607.03",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","4'557'744",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","30'223.28",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","18'230'976",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","554'134",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","138.29",
"32","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 261526 excessive sectors (22% of the total 1207720 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.89"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","32'918",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","38.58",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","38.58",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","21.63",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.78",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","42.00",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","27'497.62",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","23.96",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.10",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.95",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.82",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.11",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.82",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.38"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.55",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","24.56",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","38.58",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.83",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","18.92",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","16.31",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","15.79"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","17.94"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.84",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.16",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.32",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.63",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.32 active warps per scheduler, but only an average of 0.63 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","61.42"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","51.42",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","52.00",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","19.37",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.57",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 37.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.4% of the total average of 51.4 cycles between issuing two instructions.","global","61.42"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 19.4 threads being active per cycle. This is further reduced to 18.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","10.05"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7'565.57",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","3'994'620",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","7'650.86",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","4'039'653",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 276318 fused and 214914 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.573"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.21",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.18",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.42"
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","21'823.33",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'714'880",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","27'497.62",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","4'215'716",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","29'779.21",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","3'490'656",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","27'497.62",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","4'215'716",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","27'477.22",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","16'862'864",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","472'836",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","116.30",
"33","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 220998 excessive sectors (22% of the total 1017225 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.79"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","29'293",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","35.56",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","35.56",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","19.23",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.61",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","38.59",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","23'603.51",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.34",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.07",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.88",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.99",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.08",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.99",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.8"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.43",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","22.06",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","35.56",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.76",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","17.61",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.97",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","14.48"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","16.52"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.39",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.61",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.08",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.08 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","64.44"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","47.76",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.40",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","19.73",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.88",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 36.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.5% of the total average of 47.8 cycles between issuing two instructions.","global","64.44"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 19.7 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","9.161"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6'286.31",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","3'319'170",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6'370.44",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","3'363'594",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 222993 fused and 173439 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.479"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","81.38",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.09",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (81.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","17.32"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","17'884",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'414'336",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","23'603.51",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","3'763'282",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","25'937.92",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","3'095'712",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","23'603.51",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","3'763'282",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","23'260.27",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","15'053'128",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.09% above the average, while the minimum instance value is 10.26% below the average.","global","7.527"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.61% above the average, while the minimum instance value is 10.34% below the average.","global","7.84"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.09% above the average, while the minimum instance value is 10.26% below the average.","global","7.527"
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","389'886",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","93.85",
"34","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 177590 excessive sectors (22% of the total 820537 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.41"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","26'259",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","30.80",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","30.80",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","17.28",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.40",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","33.83",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20'751.34",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.67",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.77",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.48",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.48",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","87.96"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.24",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.12",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","30.80",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.34",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","16.11",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.98",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 86.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","12.77"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.1"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.84",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.16",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.89",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.89 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.2"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","57.64",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","58.63",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.14",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.25",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 41.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 72.7% of the total average of 57.6 cycles between issuing two instructions.","global","69.2"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.1 threads being active per cycle. This is further reduced to 19.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.834"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4'995.59",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","2'637'673",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5'080.82",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","2'682'672",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 171333 fused and 133259 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.292"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.06",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.72",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.56"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","13'903",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'166'656",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","20'751.34",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","3'410'098",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","23'876.05",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","2'786'400",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","20'751.34",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","3'410'098",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","19'663.10",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","13'640'392",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.94% above the average, while the minimum instance value is 10.46% below the average.","global","9.588"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.58% above the average, while the minimum instance value is 11.57% below the average.","global","11.1"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.94% above the average, while the minimum instance value is 10.46% below the average.","global","9.588"
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","307'059",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","72.11",
"35","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 136389 excessive sectors (22% of the total 630336 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.8"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","21'462",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","28.74",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","28.74",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","14.11",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","21.21",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","32.16",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","17'557.42",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","17.68",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.69",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.48",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.90",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.48",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","88.76"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.15",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.91",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","28.74",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.12",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","15.67",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.62",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 86.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","12.17"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.52"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.87",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.13",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.40",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.40 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","71.26"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","58.57",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","59.84",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.52",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.66",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 44.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.1% of the total average of 58.6 cycles between issuing two instructions.","global","71.26"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.5 threads being active per cycle. This is further reduced to 19.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.818"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3'862.31",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","2'039'298",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3'946.16",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","2'083'570",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 128250 fused and 99750 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.143"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.32",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.97",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.34"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","10'591.67",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'768'960",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","17'557.42",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'945'864",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","18'946.89",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","2'272'416",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","17'557.42",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'945'864",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","17'252.16",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","11'783'456",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.24% above the average, while the minimum instance value is 27.96% below the average.","global","5.697"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.34% above the average, while the minimum instance value is 28.11% below the average.","global","8.767"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.24% above the average, while the minimum instance value is 27.96% below the average.","global","5.697"
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","235'254",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","53.98",
"36","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 102203 excessive sectors (22% of the total 472038 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.33"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","18'573",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","23.67",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","23.67",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","12.22",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","20.59",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.35",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","13'099.38",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","15.25",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.87",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.59",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.39",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.90",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.39",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","88.54"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","950.37",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","14.99",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","23.67",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.73",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","15.61",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.94",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 86.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","10.01"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","10.52"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.79",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.21",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.22",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.57",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.22 active warps per scheduler, but only an average of 0.57 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","76.33"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","62.37",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","64.25",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.12",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.30",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 45.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.3% of the total average of 62.4 cycles between issuing two instructions.","global","73.28"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.1 threads being active per cycle. This is further reduced to 20.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","5.576"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'846.68",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'503'048",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'932.38",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'548'296",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 89640 fused and 69720 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.071"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.90",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.69",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.77"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","7'563.33",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'533'824",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","13'099.38",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'537'656",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","15'810.78",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'971'552",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","13'099.38",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'537'656",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","12'864.19",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","10'150'624",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.20% above the average, while the minimum instance value is 13.88% below the average.","global","11.72"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 21.27% above the average, while the minimum instance value is 14.56% below the average.","global","14.23"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.20% above the average, while the minimum instance value is 13.88% below the average.","global","11.72"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.74% above the average, while the minimum instance value is 9.16% below the average.","global","5.191"
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","170'904",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","37.73",
"37","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 71418 excessive sectors (22% of the total 330013 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","16.66"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'631",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.42",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.42",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.63",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.03",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.12",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","11'027.80",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","14.79",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.74",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.57",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","19.16",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.77",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","19.16",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","89.81"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","817.49",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","13.14",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.42",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.58",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","16.30",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.19",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.826"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","9.857"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","19.40",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.19",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","80.60",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.20",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.47",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.20 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","79.58"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","62.89",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","65.37",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.02",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.27",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 45.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 71.9% of the total average of 62.9 cycles between issuing two instructions.","global","71.93"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.0 threads being active per cycle. This is further reduced to 21.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","4.958"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'033.00",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'073'423",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'113.38",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'115'864",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 58707 fused and 45661 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.8332"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","73.98",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","47.35",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (74.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","24.85"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'126.33",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'204'864",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","11'027.80",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'886'134",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","10'653.77",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'549'344",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","11'027.80",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'886'134",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","10'895.67",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'544'536",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.62% above the average, while the minimum instance value is 12.93% below the average.","global","7.012"
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","119'349",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","24.71",
"38","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 47387 excessive sectors (22% of the total 216800 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.43"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13'292",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","14.80",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","14.80",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.74",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","14.48",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.71",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'294.95",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.48",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.64",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.48",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","16.80",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.67",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","16.80",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","90.63"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","592.76",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","10.76",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","14.80",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.53",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","19.12",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.69",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","6.734"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","8.068"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","18.33",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.18",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","81.67",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.87",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.42",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.87 active warps per scheduler, but only an average of 0.42 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","81.67"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","53.85",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","56.47",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","23.17",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","22.51",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 34.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 64.5% of the total average of 53.9 cycles between issuing two instructions.","global","64.53"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 23.2 threads being active per cycle. This is further reduced to 22.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","3.7"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'488.96",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","786'173",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'561.36",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","824'400",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 38025 fused and 29575 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.6403"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.41",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.11",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.69"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'371.33",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'093'120",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'294.95",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'651'474",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","9'048.27",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'411'680",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'294.95",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'651'474",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'518.27",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'605'896",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.16% above the average, while the minimum instance value is 13.45% below the average.","global","9.78"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.70% above the average, while the minimum instance value is 13.20% below the average.","global","8.65"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.16% above the average, while the minimum instance value is 13.45% below the average.","global","9.78"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.01% above the average, while the minimum instance value is 11.08% below the average.","global","6.161"
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","84'879",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","16.00",
"39","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 30200 excessive sectors (22% of the total 139925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.28"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'738",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.44",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","9.44",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.35",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","11.54",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","11.95",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'063.77",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.25",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.54",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","14.22",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.57",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","14.22",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","91.51"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","378.27",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.50",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","9.44",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.45",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","22.95",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.26",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","4.559"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.627"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","14.74",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.15",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","85.26",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","6.80",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.33",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 6.80 active warps per scheduler, but only an average of 0.33 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","85.26"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.15",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.71",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","24.77",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.23",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.6% of the total average of 46.2 cycles between issuing two instructions.","global","59.61"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'086.50",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","573'673",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'146.65",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","605'431",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 22725 fused and 17675 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4411"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 56.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","42.64",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.29",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (42.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.68"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'056.83",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'046'016",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'063.77",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'637'006",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'663.93",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'355'712",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'063.77",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'637'006",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'780",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'548'024",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 17.40% above the average, while the minimum instance value is 73.16% below the average.","global","11.31"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 19.59% above the average, while the minimum instance value is 72.92% below the average.","global","12.29"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 17.40% above the average, while the minimum instance value is 73.16% below the average.","global","11.31"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.18% above the average, while the minimum instance value is 13.46% below the average.","global","7.151"
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","59'379",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","9.56",
"40","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 18250 excessive sectors (22% of the total 83865 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.81"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'010",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.96",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.96",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.87",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","12.50",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","7.78",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5'417.80",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.56",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.62",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","16.35",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.65",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","16.35",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","89.49"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","239.38",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.68",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.96",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.66",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.89",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","7.56",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.963"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.262"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","16.70",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.17",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","83.30",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.18",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.34",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.18 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","83.3"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.99",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.42",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.52",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.11",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 18.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 43.6% of the total average of 43.0 cycles between issuing two instructions.","global","43.57"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","838.40",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","442'673",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","885.92",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","467'768",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 13293 fused and 10339 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.384"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 56.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","43.24",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.68",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (43.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.07"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","1'226.83",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","987'648",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5'417.80",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'573'132",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'749.34",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'280'256",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5'417.80",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'573'132",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5'305.56",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'292'528",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.52% above the average, while the minimum instance value is 66.72% below the average.","global","18.42"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.96% above the average, while the minimum instance value is 68.58% below the average.","global","18.68"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.52% above the average, while the minimum instance value is 66.72% below the average.","global","18.42"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.69% above the average, while the minimum instance value is 12.24% below the average.","global","8.538"
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","43'659",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","5.59",
"41","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 10693 excessive sectors (22% of the total 49070 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.66"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'410",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.43",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.27",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.16",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","13.49",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.42",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'850.40",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.97",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.71",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.23",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","18.80",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.75",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","18.80",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","87.09"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","130.95",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.43",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.12",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.90",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.40",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.97",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.954"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.325"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","19.46",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.19",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","80.54",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.81",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.40",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.81 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","80.54"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.12",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.56",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.27",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.99",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 33.0% of the total average of 40.1 cycles between issuing two instructions.","global","32.96"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","682.38",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","360'298",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","723.83",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","382'181",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 7362 fused and 5726 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2993"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 51.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","47.37",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","30.32",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (47.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","51.87"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","695.67",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'019'904",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'850.40",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'546'266",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6'544.96",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'325'280",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'850.40",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'546'266",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'718.71",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'185'064",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.32% above the average, while the minimum instance value is 55.59% below the average.","global","19.5"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 58.80% above the average, while the minimum instance value is 57.62% below the average.","global","18.67"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.32% above the average, while the minimum instance value is 55.59% below the average.","global","19.5"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.41% above the average, while the minimum instance value is 21.13% below the average.","global","7.781"
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","33'774",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.10",
"42","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6050 excessive sectors (22% of the total 27308 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.5"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'371",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.83",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.88",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.46",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","14.92",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.49",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'844.31",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.91",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.83",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.21",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.03",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.88",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.03",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.07"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","75.54",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.83",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.66",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.71",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","31.27",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.91",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.678"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.872"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.65",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.35",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.79",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.50",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.79 active warps per scheduler, but only an average of 0.50 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.35"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","37.16",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.64",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.79",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.63",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","587.45",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","310'173",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","626.70",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","330'898",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 3753 fused and 2919 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2065"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 45.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.05",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.59",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","45.09"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","366.67",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","934'400",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'844.31",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'463'398",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'723.93",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'215'936",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'844.31",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'463'398",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'649.72",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'853'592",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.40% above the average, while the minimum instance value is 43.40% below the average.","global","17.55"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.71% above the average, while the minimum instance value is 41.65% below the average.","global","16.42"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.40% above the average, while the minimum instance value is 43.40% below the average.","global","17.55"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 17.42% above the average, while the minimum instance value is 23.08% below the average.","global","7.87"
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","27'759",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.58",
"43","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 3026 excessive sectors (22% of the total 13863 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.864"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'677",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.45",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.16",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.01",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","15.36",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.76",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'489.52",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.71",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.20",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.28",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.93",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.28",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.59"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","46.72",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.45",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.35",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.81",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.73",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.71",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.515"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.586"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.51",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.49",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.10",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.10 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.49"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.69",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.93",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.64",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.54",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","545.31",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","287'923",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","579.54",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","305'995",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2151 fused and 1673 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1352"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 40.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","58.87",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.68",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (58.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","40.19"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","213.17",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","878'592",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'489.52",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'464'242",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'773.26",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'141'344",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'489.52",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'464'242",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'271.91",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'856'968",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.79% above the average, while the minimum instance value is 32.80% below the average.","global","16.11"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 73.49% above the average, while the minimum instance value is 35.82% below the average.","global","15.05"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.79% above the average, while the minimum instance value is 32.80% below the average.","global","16.11"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 25.77% above the average, while the minimum instance value is 33.09% below the average.","global","10.34"
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","25'089",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.91",
"44","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1747 excessive sectors (22% of the total 7956 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.816"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'889",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.40",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.53",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.17",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","15.46",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.70",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'271.71",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.05",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.91",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.20",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.07",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.07",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.55"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","21.04",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.40",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.35",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.47",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","39.73",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","7.05",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.493"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.55"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.26",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.74",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.64",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.64 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.74"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.12",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.29",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.35",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.30",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","514.06",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","271'423",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","546.77",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","288'695",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 963 fused and 749 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.06635"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 33.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","65.47",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.90",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (65.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","33.49"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","98.17",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","896'000",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'271.71",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'363'624",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'400.95",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'165'056",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'271.71",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'363'624",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'934.69",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'454'496",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.26% above the average, while the minimum instance value is 21.03% below the average.","global","15.67"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.11% above the average, while the minimum instance value is 26.55% below the average.","global","14.07"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.26% above the average, while the minimum instance value is 21.03% below the average.","global","15.67"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 27.44% above the average, while the minimum instance value is 40.10% below the average.","global","7.689"
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","23'109",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.41",
"45","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 783 excessive sectors (22% of the total 3565 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.155"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'969",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.21",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.32",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.56",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.54",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.50",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'930.60",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.84",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.19",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.59",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.10",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.59",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","13.03",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.21",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.18",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.10",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","47.44",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.84",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.44"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.405"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.59",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.41",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.45",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.45 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.41"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.25",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.38",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.64",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.0 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 32.1% of the total average of 34.3 cycles between issuing two instructions.","global","32.06"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","501.51",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","264'798",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","532.62",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","281'224",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 486 fused and 378 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0394"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 37.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","61.86",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.59",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (61.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","37.16"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","55.67",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","822'272",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'930.60",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'393'710",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'015.71",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'066'080",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'930.60",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'393'710",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'930.79",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'574'840",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.88% above the average, while the minimum instance value is 23.55% below the average.","global","13.87"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.93% above the average, while the minimum instance value is 22.05% below the average.","global","13.88"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.88% above the average, while the minimum instance value is 23.55% below the average.","global","13.87"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 34.71% above the average, while the minimum instance value is 55.63% below the average.","global","9.426"
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'314",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.20",
"46","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 421 excessive sectors (23% of the total 1820 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.282"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'137",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.37",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.19",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.66",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.79",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.39",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'867.76",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.30",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.06",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.20",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.25",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.13",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.25",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.0%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","7.42",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.37",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.36",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.94",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","67.10",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","7.30",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.5 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.419"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.53"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.65",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.35",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.12",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.12 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.35"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.13",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.37",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.83",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.82",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","495.12",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","261'423",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","527.55",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","278'546",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 243 fused and 189 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.02036"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.70",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.77",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.29"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","32.17",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","834'048",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'867.76",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'300'252",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'313.41",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'084'704",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'867.76",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'300'252",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'779.27",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'201'008",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.64% above the average, while the minimum instance value is 16.48% below the average.","global","14.34"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.89% above the average, while the minimum instance value is 17.38% below the average.","global","13.71"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.64% above the average, while the minimum instance value is 16.48% below the average.","global","14.34"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.74% above the average, while the minimum instance value is 86.30% below the average.","global","8.547"
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'909",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.10",
"47","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 170 excessive sectors (19% of the total 872 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","3.992"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'767",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.22",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.12",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.43",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.29",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.40",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'796.04",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.04",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.19",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.03",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.03",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.7%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","4.62",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.22",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.22",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.83",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","45.37",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","7.04",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.5 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.463"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.419"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.13",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.87",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.21",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.21 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.87"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.90",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.98",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.93",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","491.33",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'423",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","521.45",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","275'324",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 99 fused and 77 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.008627"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.70",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.13",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.3"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","19.33",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","802'176",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'796.04",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'344'224",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'523.77",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'044'288",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'796.04",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'344'224",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'730.87",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'376'896",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 76.20% above the average, while the minimum instance value is 13.48% below the average.","global","13.44"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 77.76% above the average, while the minimum instance value is 19.58% below the average.","global","13.22"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 76.20% above the average, while the minimum instance value is 13.48% below the average.","global","13.44"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 49.64% above the average, while the minimum instance value is 79.79% below the average.","global","6.954"
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'669",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.04",
"48","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 90 excessive sectors (24% of the total 376 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","3.353"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'812",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.30",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.10",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.79",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.31",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.39",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'788.08",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.41",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.10",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.21",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.17",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.21",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","4.02",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.30",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.30",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.66",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","60.58",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.41",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.8 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.046"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.226"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.77",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.23",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.00",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.00 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.23"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.51",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.63",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.96",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.95",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 33.0% of the total average of 32.5 cycles between issuing two instructions.","global","32.97"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","490.38",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'923",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","522.31",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","275'780",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 63 fused and 49 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.005514"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.55",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.31",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.43"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","15.17",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","724'992",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'788.08",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'004'840",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'015.08",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","943'200",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'788.08",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'004'840",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'697.32",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'019'360",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.43% above the average, while the minimum instance value is 12.20% below the average.","global","15.37"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.71% above the average, while the minimum instance value is 13.27% below the average.","global","14.87"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.43% above the average, while the minimum instance value is 12.20% below the average.","global","15.37"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 60.58% above the average, while the minimum instance value is 85.81% below the average.","global","6.259"
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'609",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.03",
"49","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 67 excessive sectors (27% of the total 249 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","2.78"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'935",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.44",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.05",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.86",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.62",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.30",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'750.69",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.75",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.12",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.72",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.72",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.3%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.14",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.44",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.44",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.79",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","69.24",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.75",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 20.7 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.566"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.33"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.51",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.49",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.75",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.75 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.49"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.96",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.01",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 9.9 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 30.9% of the total average of 32.0 cycles between issuing two instructions.","global","30.87"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","488.96",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'173",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","520.31",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'725",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9 fused and 7 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0008046"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.24",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.47",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.76"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","8.17",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","735'872",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'750.69",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","969'314",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","597.10",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","956'544",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'750.69",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","969'314",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'705.63",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'877'256",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.34% above the average, while the minimum instance value is 11.06% below the average.","global","15.34"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.38% above the average, while the minimum instance value is 17.10% below the average.","global","15.19"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 64.34% above the average, while the minimum instance value is 11.06% below the average.","global","15.34"
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'519",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.00",
"50","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 3 excessive sectors (10% of the total 29 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","0.6199"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'899",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.40",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.06",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.86",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.81",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.28",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'734.36",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.66",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.13",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.03",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.03",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.5%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.27",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.40",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.40",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.79",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","69.13",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.66",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 20.7 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.553"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.303"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.85",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.15",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.10",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.10 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.15"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.76",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.89",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.1 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 30.8% of the total average of 32.8 cycles between issuing two instructions.","global","30.83"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","488.96",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'173",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","520.76",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'962",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9 fused and 7 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0008122"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.24",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.47",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.76"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","8.67",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","731'136",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'734.36",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","977'964",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","564.94",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","953'280",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'734.36",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","977'964",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'688.30",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'911'856",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.38% above the average, while the minimum instance value is 15.19% below the average.","global","15.77"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.73% above the average, while the minimum instance value is 13.64% below the average.","global","15.21"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.38% above the average, while the minimum instance value is 15.19% below the average.","global","15.77"
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'519",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.00",
"51","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 3 excessive sectors (10% of the total 29 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","0.5885"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'584",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.22",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.09",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.30",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.19",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.31",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'799.80",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.24",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.18",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.17",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.18",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.7%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","3.61",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.22",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.22",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.66",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","63.73",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.24",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.8 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.008"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.165"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.79",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.21",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.43",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.43 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.21"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.61",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.67",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.93",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","490.76",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'121",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","525.20",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","277'303",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 87 fused and 55 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 19% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.006318"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.27",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.49",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.73"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","14.83",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","789'120",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'799.80",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'024'174",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'040.50",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'027'008",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'799.80",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'024'174",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'705.64",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'096'696",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.66% above the average, while the minimum instance value is 21.05% below the average.","global","15.7"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.32% above the average, while the minimum instance value is 20.38% below the average.","global","15.02"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.66% above the average, while the minimum instance value is 21.05% below the average.","global","15.7"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 55.32% above the average, while the minimum instance value is 78.86% below the average.","global","5.381"
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'633",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","81.82",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.03",
"52","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 67 excessive sectors (27% of the total 249 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","2.617"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'650",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.45",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.11",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.37",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.40",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.38",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'784.46",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.54",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.10",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.21",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.63",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.63",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.9%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","4.58",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.45",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.45",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.83",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","71.60",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","7.54",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.5 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.566"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.589"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.80",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.20",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.78",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.78 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.2"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.49",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.63",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.91",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.91",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","491.58",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'555",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","528.69",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","279'148",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 115 fused and 81 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.009255"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.00",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.96",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.99"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","19",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","793'344",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'784.46",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'255'868",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'401.40",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'033'152",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'784.46",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'255'868",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'716.39",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'023'472",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.57% above the average, while the minimum instance value is 13.76% below the average.","global","14.17"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.43% above the average, while the minimum instance value is 15.64% below the average.","global","13.61"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.57% above the average, while the minimum instance value is 13.76% below the average.","global","14.17"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 56.84% above the average, while the minimum instance value is 91.87% below the average.","global","7.402"
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'685",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.43",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.04",
"53","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 90 excessive sectors (24% of the total 376 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","3.117"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'622",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.05",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.18",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.98",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.67",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.38",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'880.13",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.61",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.05",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.18",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.17",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.13",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.17",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","7.08",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.05",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.04",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.94",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","65.24",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.61",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.5 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.285"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.289"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.56",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.44",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.57",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.57 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.44"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.51",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.81",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.80",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.79",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.0 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 32.8% of the total average of 33.5 cycles between issuing two instructions.","global","32.79"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","495.62",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","261'687",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","529.65",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","279'657",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 275 fused and 197 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.02132"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.55",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.03",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.45"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","32.17",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","873'600",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'880.13",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'436'634",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'469.65",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'137'312",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'880.13",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'436'634",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'854.57",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'746'536",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.83% above the average, while the minimum instance value is 14.42% below the average.","global","13.1"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 77.78% above the average, while the minimum instance value is 19.33% below the average.","global","13.25"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 75.83% above the average, while the minimum instance value is 14.42% below the average.","global","13.1"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 45.70% above the average, while the minimum instance value is 87.37% below the average.","global","9.526"
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'941",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.87",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.10",
"54","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 170 excessive sectors (19% of the total 872 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","4.064"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'155",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.25",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.32",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.66",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.67",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.47",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'915.42",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.93",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.05",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.19",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.02",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.12",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.02",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.6%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","12.88",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.25",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.22",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.10",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","57.02",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.93",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.458"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.436"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.01",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.99",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.15",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.15 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.99"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.50",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.24",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.60",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.57",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 14.8 cycles being stalled waiting to be selected to fetch an instruction or waiting on an instruction cache miss. A high number of warps not having an instruction fetched is typical for very short kernels with less than one full wave of work in the grid. Excessively jumping across large blocks of assembly code can also lead to more warps stalled for this reason, if this causes misses in the instruction cache. See also the related Branch Resolving state. This stall type represents about 36.5% of the total average of 40.5 cycles between issuing two instructions.","global","36.51"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","502.64",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","265'392",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","536.61",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","283'328",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 558 fused and 396 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.04211"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.68",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.12",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.32"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","55.83",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","835'328",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'915.42",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'375'834",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'198.15",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'087'008",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'915.42",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'375'834",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'788.30",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'503'336",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 76.42% above the average, while the minimum instance value is 19.29% below the average.","global","14.04"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 77.49% above the average, while the minimum instance value is 21.43% below the average.","global","13.29"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 76.42% above the average, while the minimum instance value is 19.29% below the average.","global","14.04"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 37.96% above the average, while the minimum instance value is 53.10% below the average.","global","10.72"
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'386",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.18",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.20",
"55","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 421 excessive sectors (23% of the total 1820 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.534"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'618",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.26",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.54",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.98",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","14.81",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.82",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'370.92",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.76",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.87",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.19",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.07",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.92",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.07",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.25"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","21.47",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.26",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.21",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.49",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","43.52",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.76",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.431"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.443"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.58",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.42",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.38",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.38 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.42"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.02",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.05",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.24",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.19",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","516.06",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","272'479",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","546.95",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","288'788",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1091 fused and 781 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.06702"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 33.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","65.27",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.78",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (65.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","33.69"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","97.50",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","872'448",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'370.92",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'423'160",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'020.43",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'135'584",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'370.92",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'423'160",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'983.12",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'692'640",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 73.01% above the average, while the minimum instance value is 20.41% below the average.","global","16.05"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 76.09% above the average, while the minimum instance value is 27.39% below the average.","global","14"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 73.01% above the average, while the minimum instance value is 20.41% below the average.","global","16.05"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 27.38% above the average, while the minimum instance value is 35.98% below the average.","global","9.305"
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","23'237",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.89",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.41",
"56","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 783 excessive sectors (22% of the total 3565 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","7.465"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'017",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.29",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.13",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.23",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.18",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.56",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'363.97",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.42",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.19",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.78",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.78",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.45"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","45.20",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.29",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.20",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.80",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","34.63",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.42",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.447"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.47"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.69",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.31",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.00",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.48",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.00 active warps per scheduler, but only an average of 0.48 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","77.31"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.28",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.54",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.35",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.25",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 31.5% of the total average of 35.3 cycles between issuing two instructions.","global","31.45"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","550.56",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","290'695",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","585.85",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","309'327",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2487 fused and 1757 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1515"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.07",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.88",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.04"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","212.83",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","903'680",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'363.97",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'533'020",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'827.06",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'177'344",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'363.97",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'533'020",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'582.26",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'132'080",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 72.68% above the average, while the minimum instance value is 34.14% below the average.","global","14.79"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 71.22% above the average, while the minimum instance value is 33.08% below the average.","global","15.83"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 72.68% above the average, while the minimum instance value is 34.14% below the average.","global","14.79"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 22.44% above the average, while the minimum instance value is 38.39% below the average.","global","8.834"
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","25'425",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.33",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.91",
"57","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1747 excessive sectors (22% of the total 7956 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.643"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'753",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.64",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.82",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.74",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","14.87",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.43",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'855",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.58",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.83",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.20",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.08",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.88",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.08",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.9"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","72.69",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.64",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.48",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.71",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","30.88",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.58",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.595"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.73"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.53",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.47",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.63",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.51",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.63 active warps per scheduler, but only an average of 0.51 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.47"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.68",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","38.89",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.45",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.28",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","594.57",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","313'935",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","630.49",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","332'898",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 4209 fused and 3033 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.216"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 46.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","52.57",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.65",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (52.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","46.59"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","366.50",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","965'760",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'855",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'539'462",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'834.14",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'254'528",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'855",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'539'462",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'679.61",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'157'848",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.72% above the average, while the minimum instance value is 41.75% below the average.","global","17.07"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 70.13% above the average, while the minimum instance value is 45.25% below the average.","global","16.11"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.72% above the average, while the minimum instance value is 41.75% below the average.","global","17.07"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 18.05% above the average, while the minimum instance value is 21.38% below the average.","global","8.058"
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","28'215",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.67",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.58",
"58","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 3026 excessive sectors (22% of the total 13863 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.745"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'162",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.21",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","3.35",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","12.58",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.50",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","4'127.60",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.64",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.68",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.23",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","17.97",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.72",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","17.97",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","87.53"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","133.63",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.21",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.92",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.90",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.00",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","6.64",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.855"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.157"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","19.52",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.20",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","80.48",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.71",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.43",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.71 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","80.48"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.51",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.80",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.54",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.26",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 12.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 32.0% of the total average of 39.5 cycles between issuing two instructions.","global","32"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","701.26",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","370'264",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","741.93",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","391'739",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8570 fused and 6028 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2978"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.42",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.27",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (50.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.78"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","696",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","996'608",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","4'127.60",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'628'316",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6'917.16",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'295'712",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","4'127.60",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'628'316",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'801.29",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'513'264",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.10% above the average, while the minimum instance value is 55.30% below the average.","global","19.78"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 60.65% above the average, while the minimum instance value is 59.54% below the average.","global","18.69"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.10% above the average, while the minimum instance value is 55.30% below the average.","global","19.78"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.94% above the average, while the minimum instance value is 15.15% below the average.","global","7.146"
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","34'982",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.48",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.10",
"59","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6050 excessive sectors (22% of the total 27308 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.35"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'870",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.03",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","6.03",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.81",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","12.46",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","7.92",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5'422.33",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","7.49",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.64",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.28",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","17.06",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.68",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","17.06",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","89.01"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","241.41",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.47",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.03",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.65",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.83",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","7.29",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.018"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.104"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","16.35",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.16",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","83.65",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","6.90",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.32",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 6.90 active warps per scheduler, but only an average of 0.32 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","83.65"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.20",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.04",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","25.69",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.29",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 18.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 44.5% of the total average of 42.2 cycles between issuing two instructions.","global","44.48"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","866.77",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","457'655",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","925.19",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","488'499",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 15109 fused and 10793 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4051"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 56.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","42.96",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.49",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (43.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.36"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","1'227.17",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","976'640",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5'422.33",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'629'846",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'108.90",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'266'912",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5'422.33",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'629'846",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5'658.79",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'519'384",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.94% above the average, while the minimum instance value is 67.63% below the average.","global","18.86"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 41.51% above the average, while the minimum instance value is 70.36% below the average.","global","19.02"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.94% above the average, while the minimum instance value is 67.63% below the average.","global","18.86"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 17.58% above the average, while the minimum instance value is 20.83% below the average.","global","9.469"
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","45'475",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.96",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","5.59",
"60","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 10693 excessive sectors (22% of the total 49070 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.74"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.53",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13'247",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.04",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","9.04",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.67",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","11.49",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","11.51",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'119.16",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.32",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.56",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","14.68",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.59",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","14.68",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","91.04"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","364.22",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.29",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","9.04",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.45",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.03",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.04",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","4.391"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.469"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","15.32",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.15",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","84.68",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.06",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.34",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.06 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","84.68"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.11",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.54",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","23.81",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","23.29",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 55.4% of the total average of 46.1 cycles between issuing two instructions.","global","55.43"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 23.8 threads being active per cycle. This is further reduced to 23.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","2.536"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'132.25",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","597'829",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'191.91",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","629'328",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 25653 fused and 18407 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4611"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 53.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","45.36",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","29.03",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (45.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","53.92"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'056.33",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'091'584",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'119.16",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'688'726",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'101.25",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'408'608",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'119.16",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'688'726",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'781.95",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'754'904",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.46% above the average, while the minimum instance value is 70.29% below the average.","global","10.45"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 21.70% above the average, while the minimum instance value is 73.61% below the average.","global","13.2"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.46% above the average, while the minimum instance value is 70.29% below the average.","global","10.45"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.51% above the average, while the minimum instance value is 13.22% below the average.","global","7.461"
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","62'307",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.81",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","9.56",
"61","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 18250 excessive sectors (22% of the total 83865 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.01"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","13'890",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","14.16",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","14.16",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.12",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","13.43",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.07",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","10'015.72",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","11.98",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.62",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.46",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","16.25",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.65",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","16.25",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","90.65"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","567.94",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","9.90",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","14.16",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.53",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","19.26",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.99",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","6.492"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","7.426"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","17.21",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.17",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","82.79",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.54",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.42",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.54 active warps per scheduler, but only an average of 0.42 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","82.79"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","49.60",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","51.81",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.19",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.55",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 34.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 69.6% of the total average of 49.6 cycles between issuing two instructions.","global","69.57"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.2 threads being active per cycle. This is further reduced to 21.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","3.912"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'557.96",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","822'605",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'627.49",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","859'314",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 42441 fused and 30679 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.6224"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.90",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.42",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.2"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'372.17",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'142'784",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","10'015.72",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'792'598",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","9'431.81",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'474'752",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","10'015.72",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'792'598",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'456.36",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'170'392",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.58% above the average, while the minimum instance value is 12.12% below the average.","global","9.278"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.85% above the average, while the minimum instance value is 11.04% below the average.","global","8.254"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.58% above the average, while the minimum instance value is 12.12% below the average.","global","9.278"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.48% above the average, while the minimum instance value is 9.85% below the average.","global","5.823"
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","89'295",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.56",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","16.00",
"62","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 30200 excessive sectors (22% of the total 139925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.25"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'823",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.20",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.20",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.76",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.59",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.04",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","11'328.98",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","15.40",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.75",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.59",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","19.51",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.78",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","19.51",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","89.28"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","806.35",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","13.09",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.20",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.59",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","16.40",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.19",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 87.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.799"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","9.818"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","20.58",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","79.42",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.83",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.50",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.83 active warps per scheduler, but only an average of 0.50 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","79.42"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","62.36",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","64.74",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.07",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.35",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 42.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 67.6% of the total average of 62.4 cycles between issuing two instructions.","global","67.58"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.1 threads being active per cycle. This is further reduced to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","5.605"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'129.00",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'124'111",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'210.31",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'167'044",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 64851 fused and 47197 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.8458"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","73.29",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","46.91",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (73.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","25.55"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'123.67",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'217'792",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","11'328.98",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'894'600",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","10'452.32",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'569'888",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","11'328.98",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'894'600",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","10'742.47",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'578'400",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.95% above the average, while the minimum instance value is 5.85% below the average.","global","5.203"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.00% above the average, while the minimum instance value is 13.11% below the average.","global","7.03"
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","125'493",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.33",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","24.71",
"63","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 47387 excessive sectors (22% of the total 216800 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.97"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","19'028",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","23.09",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","23.09",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","12.51",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","19.73",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.88",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","13'681.75",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","15.53",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.87",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.60",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.45",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.90",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.45",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","88.05"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","928.43",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","14.59",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","23.09",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.71",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","15.60",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.66",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 86.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.827"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","10.24"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.14",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.86",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.28",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.60",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.28 active warps per scheduler, but only an average of 0.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","76.86"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","57.38",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","58.97",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.16",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.38",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 42.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.6% of the total average of 57.4 cycles between issuing two instructions.","global","74.6"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.2 threads being active per cycle. This is further reduced to 19.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.124"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'988.31",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'577'826",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3'071.41",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'621'705",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 98704 fused and 71986 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.068"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.48",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.07",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.17"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","7'562.83",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'571'840",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","13'681.75",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'610'826",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","16'312.95",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","2'017'632",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","13'681.75",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'610'826",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","13'271.97",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","10'443'304",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 20.89% above the average, while the minimum instance value is 13.61% below the average.","global","14.45"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.18% above the average, while the minimum instance value is 12.55% below the average.","global","14.88"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 20.89% above the average, while the minimum instance value is 13.61% below the average.","global","14.45"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.77% above the average, while the minimum instance value is 6.50% below the average.","global","6.034"
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","179'968",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.26",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","37.73",
"64","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 71418 excessive sectors (22% of the total 330013 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","16.8"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","21'475",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","28.70",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","28.70",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","14.14",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","20.49",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","32.26",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","18'181.23",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.25",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.71",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.74",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.91",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.74",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","88.2"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.15",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.98",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","28.70",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.12",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","15.65",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.45",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 86.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","12.2"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.33"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.64",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.36",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.78",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.55",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.78 active warps per scheduler, but only an average of 0.55 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","71.3"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","56.45",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","57.67",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","19.63",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.80",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 41.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.7% of the total average of 56.5 cycles between issuing two instructions.","global","71.3"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 19.6 threads being active per cycle. This is further reduced to 18.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.525"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4'046.68",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","2'136'648",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4'133.62",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","2'182'549",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 140050 fused and 102700 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.146"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.33",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.69",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.36"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","10'589.33",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'771'008",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","18'181.23",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'990'016",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","18'401.31",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","2'271'264",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","18'181.23",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'990'016",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","18'260.03",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","11'960'064",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.54% above the average, while the minimum instance value is 23.37% below the average.","global","6.052"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.41% above the average, while the minimum instance value is 24.70% below the average.","global","5.976"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.54% above the average, while the minimum instance value is 23.37% below the average.","global","6.052"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 6.45% above the average, while the minimum instance value is 3.62% below the average.","global","5.017"
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","247'054",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.08",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","53.98",
"65","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 102203 excessive sectors (22% of the total 472038 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","16.84"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","26'548",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","30.48",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","30.48",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","17.44",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.66",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","33.51",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20'536.36",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.92",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.02",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.78",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.79",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.79",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.84"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.22",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.96",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","30.48",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.33",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","16.21",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.62",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 86.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","12.64"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.7"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.84",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.16",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.33",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.33 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.52"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","55.46",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","56.36",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","19.35",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.49",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 39.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 71.6% of the total average of 55.5 cycles between issuing two instructions.","global","69.52"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 19.3 threads being active per cycle. This is further reduced to 18.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","8.408"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","5'211.84",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","2'751'853",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5'295.81",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","2'796'187",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 185173 fused and 136719 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.349"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.07",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.72",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.55"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","13'894.67",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'188'288",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","20'536.36",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","3'509'856",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","23'563.58",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","2'814'720",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","20'536.36",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","3'509'856",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","20'493.28",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","14'039'424",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.71% above the average, while the minimum instance value is 10.58% below the average.","global","11.36"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.67% above the average, while the minimum instance value is 13.13% below the average.","global","10.53"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.71% above the average, while the minimum instance value is 10.58% below the average.","global","11.36"
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","320'899",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.84",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","72.11",
"66","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 136389 excessive sectors (22% of the total 630336 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.39"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","29'109",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","35.76",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","35.76",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","19.14",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.55",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","38.99",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","22'831.41",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","23.57",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.93",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.00",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.00",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.33"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.44",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","22.40",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","35.76",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.79",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","17.57",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","15.20",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","14.63"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","16.8"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.90",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.10",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.32",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.32 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","64.24"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.08",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.69",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","19.02",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.19",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 34.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.8% of the total average of 46.1 cycles between issuing two instructions.","global","64.24"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 19.0 threads being active per cycle. This is further reduced to 18.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","10.17"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6'534.93",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","3'450'444",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6'620.94",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","3'495'856",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 238905 fused and 177417 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.573"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","81.30",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.03",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (81.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","17.41"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","17'886.83",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'401'024",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","22'831.41",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","3'707'282",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","26'477.09",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","3'080'160",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","22'831.41",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","3'707'282",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","22'911.01",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","14'829'128",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 8.93% above the average, while the minimum instance value is 8.28% below the average.","global","7.256"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.61% above the average, while the minimum instance value is 9.60% below the average.","global","7.84"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 8.93% above the average, while the minimum instance value is 8.28% below the average.","global","7.256"
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","405'798",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.64",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","93.85",
"67","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 177590 excessive sectors (22% of the total 820537 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.86"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","32'693",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","38.92",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","38.92",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","21.41",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.98",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","42.30",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","27'285.34",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","25.55",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.15",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.01",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.95",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.95",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.41"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.56",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","24.83",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","38.92",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.80",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","18.56",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","16.85",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","15.89"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","18.52"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.43",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.57",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.07",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.07 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","61.08"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","47.80",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.31",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.78",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.00",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 36.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.7% of the total average of 47.8 cycles between issuing two instructions.","global","61.08"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.8 threads being active per cycle. This is further reduced to 18.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11.18"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7'815.94",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","4'126'818",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","7'899.34",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","4'170'851",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 292342 fused and 218920 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.623"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.97",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.02",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.66"
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","21'810.67",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'690'048",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","27'285.34",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","4'080'794",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","29'157.60",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","3'454'656",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","27'285.34",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","4'080'794",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","26'841.98",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","16'323'176",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","488'860",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.35",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","116.30",
"68","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 220998 excessive sectors (22% of the total 1017225 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.6"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","35'372",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.46",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.46",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","23.23",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","29.52",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","46.22",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","30'559.44",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","25.82",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.19",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.95",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.95",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.97"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.67",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","27.31",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.46",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.23",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","20.64",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","17.30",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","17.27"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","19.08"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.57",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.43",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.67",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.67 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.54"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.24",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.67",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.61",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.87",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 35.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.8% of the total average of 46.2 cycles between issuing two instructions.","global","58.54"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.6 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11.4"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9'066.85",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","4'787'298",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9'152.25",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","4'832'387",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 344421 fused and 259523 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.716"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.69",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.12",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.92"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","25'204.67",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","2'918'400",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","30'559.44",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","4'679'802",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","32'603.05",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","3'742'560",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","30'559.44",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","4'679'802",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","30'951.13",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","18'719'208",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.10% above the average, while the minimum instance value is 11.21% below the average.","global","5.254"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.10% above the average, while the minimum instance value is 11.21% below the average.","global","5.254"
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","569'974",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.13",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","138.29",
"69","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 261526 excessive sectors (22% of the total 1207720 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.11"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","39'275",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.56",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.56",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","25.82",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.37",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","47.19",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","32'729.84",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","26.83",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.24",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.06",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.32",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.32",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.35"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.67",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","28.18",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.56",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.41",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","21.31",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","18.23",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","17.6"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","20.16"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.59",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.41",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.04",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.04 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.44"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.43",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.80",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.54",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.83",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 34.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.9% of the total average of 44.4 cycles between issuing two instructions.","global","58.44"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11.88"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","10'165.77",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","5'367'528",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10'250.71",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","5'412'376",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 389601 fused and 295613 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.824"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.37",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.63",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.28"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","28'033.67",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'237'888",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","32'729.84",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'043'052",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","36'920.45",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'170'336",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","32'729.84",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'043'052",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","32'450.24",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","20'172'208",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.64% above the average, while the minimum instance value is 20.39% below the average.","global","5.685"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.78% above the average, while the minimum instance value is 19.55% below the average.","global","6.606"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.64% above the average, while the minimum instance value is 20.39% below the average.","global","5.685"
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","641'254",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.89",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","158.06",
"70","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 298863 excessive sectors (22% of the total 1380087 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.4"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","41'740",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.89",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.89",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","27.46",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","31.26",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","49.18",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","36'160.24",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","26.35",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.22",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.05",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.77",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.77",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.7"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.68",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","29.35",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.89",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.77",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","22.69",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","18.09",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.23"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","20.09"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.98",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.02",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.98",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.98 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.11"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.29",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.62",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.81",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.3% of the total average of 43.3 cycles between issuing two instructions.","global","58.11"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","11.68"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11'040.56",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","5'829'414",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","11'125.55",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","5'874'289",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 425240 fused and 324582 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.811"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.06",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.16",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.62"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","30'030.17",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'440'640",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","36'160.24",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'572'392",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","39'238.54",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'425'504",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","36'160.24",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'572'392",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","37'107.02",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","22'289'568",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.04% above the average, while the minimum instance value is 7.92% below the average.","global","5.31"
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","698'008",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.68",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","174.06",
"71","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 326349 excessive sectors (22% of the total 1515981 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.32"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","43'302",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.59",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.59",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","28.45",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.06",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","49.92",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","37'668.63",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.36",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.24",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.13",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.27",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.27",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.51"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.71",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","30.29",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.59",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.70",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","22.64",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","19.63",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.55"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","21.8"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.38",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.62",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.79",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.79 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.41"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.59",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.89",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.52",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.83",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.8% of the total average of 42.6 cycles between issuing two instructions.","global","57.41"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.56"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11'698.60",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'176'862",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","11'780.32",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'220'010",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 451638 fused and 346676 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.855"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.35",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.62",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.3"
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","31'712",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'574'016",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","37'668.63",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'483'242",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","40'995.28",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'597'632",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","37'668.63",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'483'242",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","36'378.49",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","21'932'968",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","740'716",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.47",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","186.42",
"72","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 350217 excessive sectors (22% of the total 1623541 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.46"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","44'081",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.07",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.07",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","28.99",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","32.92",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.04",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","38'224.90",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.07",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.27",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.12",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.03",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.28",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.03",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.18"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.06",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.07",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.84",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","22.81",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","19.54",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.93"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","21.64"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.78",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.22",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.02",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.02 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.93"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.11",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.42",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.53",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 72.1% of the total average of 44.1 cycles between issuing two instructions.","global","56.93"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.42"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12'157.17",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'418'986",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12'242.66",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'464'124",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 469972 fused and 362118 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.909"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.29",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.23",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.34"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","32'674.17",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'641'088",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","38'224.90",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'756'222",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","41'202.11",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'685'664",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","38'224.90",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'756'222",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","38'519.19",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'024'888",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.87% above the average, while the minimum instance value is 19.63% below the average.","global","5.182"
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","770'480",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.34",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","195.08",
"73","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 365456 excessive sectors (22% of the total 1696886 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.18"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","44'856",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.89",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.89",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","29.47",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.10",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.95",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","39'530.11",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","27.84",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.27",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.11",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.07",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.28",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.07",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.19"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.76",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.95",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.89",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.75",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.69",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","19.46",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.28"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","21.54"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.04",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.96",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.39",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.39 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.11"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.14",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.43",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.52",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 72.9% of the total average of 43.1 cycles between issuing two instructions.","global","56.11"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.32"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12'595.99",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'650'682",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12'679.11",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'694'572",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 487678 fused and 376776 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.92"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.39",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.29",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.23"
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","33'825.50",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'699'456",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","39'530.11",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'012'224",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","42'222.41",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'764'096",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","39'530.11",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'012'224",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'845.81",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'048'896",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","798'956",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.24",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","203.23",
"74","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 379281 excessive sectors (21% of the total 1765435 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.28"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'550",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.70",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.70",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.56",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.48",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","50.98",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","39'783.48",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.01",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.29",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.15",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.37",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.37",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.09"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.72",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.38",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.70",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.83",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.77",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.31",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.91"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.5"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.58",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.42",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.83",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.83 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.3"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.45",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.78",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.54",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.86",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 78.2% of the total average of 42.5 cycles between issuing two instructions.","global","57.3"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.82"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12'781.09",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'748'416",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12'879.71",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'800'487",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 494769 fused and 383237 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.94"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.10",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.10",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.54"
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34'123.67",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'835'904",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","39'783.48",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'860'378",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'727.76",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'946'112",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","39'783.48",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'860'378",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","39'530.22",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'441'512",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","810'982",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.15",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","206.97",
"75","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 386063 excessive sectors (21% of the total 1796824 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.65"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'885",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.06",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.06",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.78",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.47",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.26",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'650.47",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.33",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.28",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.27",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.27",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.14"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.75",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.06",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.88",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.82",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.62",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.82"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.78",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.22",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.79",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.79 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.94"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.06",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.32",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.53",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.86",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.2% of the total average of 42.1 cycles between issuing two instructions.","global","56.94"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.96"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'037.22",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'883'650",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'118.39",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'926'510",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 505066 fused and 391820 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.94"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.11",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.11",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.52"
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34'685.67",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'866'624",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'650.47",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'902'996",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'166.49",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'978'560",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'650.47",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'902'996",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'018.51",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'611'984",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","827'604",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.09",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","211.77",
"76","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 393541 excessive sectors (21% of the total 1835812 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.26"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'383",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.67",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.67",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.50",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.77",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.91",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","39'410.88",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.98",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.15",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.46",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.46",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.57"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.34",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.67",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.91",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","23.98",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.38",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.25"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.59"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.51",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.49",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.83",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.83 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.33"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.55",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.83",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.54",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.86",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 33.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.5% of the total average of 42.6 cycles between issuing two instructions.","global","56.33"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.8"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'101.74",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","6'917'718",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'185.94",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","6'962'174",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 507538 fused and 394072 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.013"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.24",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.19",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.39"
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34'825.33",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'827'456",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","39'410.88",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'005'822",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'142.02",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'928'928",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","39'410.88",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'005'822",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'556.61",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'023'288",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","831'796",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.06",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","213.07",
"77","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 397189 excessive sectors (21% of the total 1847999 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.48"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'591",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.88",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.88",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.59",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.44",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.04",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'527.33",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.32",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.28",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.14",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.14",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.23"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.76",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.50",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.88",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.97",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.24",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.65",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.27"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.87"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.91",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.09",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.78",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.78 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.12"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.88",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.14",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.53",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.86",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.7% of the total average of 41.9 cycles between issuing two instructions.","global","56.12"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.96"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'265.49",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'004'178",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'348.85",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'048'192",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 514202 fused and 399500 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.936"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.54",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.39",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.08"
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'114.50",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'841'280",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'527.33",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'010'584",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","44'731.17",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'946'304",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'527.33",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'010'584",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'555.53",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'042'336",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","842'420",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.04",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","216.07",
"78","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 401012 excessive sectors (21% of the total 1871269 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.6"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'691",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.13",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.13",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.62",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.45",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.52",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'514.03",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.25",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.10",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.10",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.76"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.77",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.68",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.13",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.99",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.33",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.61",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.45"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.83"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.90",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.10",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.85",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.85 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","55.87"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.10",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.37",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.53",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.86",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.0% of the total average of 42.1 cycles between issuing two instructions.","global","55.87"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.93"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'325.26",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'035'738",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'410.52",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'080'753",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 516582 fused and 401520 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.995"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.22",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.82",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.4"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'365.33",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'846'912",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'514.03",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'051'934",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'087.38",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'954'656",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'514.03",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'051'934",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'764.81",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'207'736",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.77% above the average, while the minimum instance value is 18.27% below the average.","global","5.128"
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","846'300",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.02",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","217.20",
"79","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 402852 excessive sectors (21% of the total 1880754 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.71"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'471",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.46",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.46",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.17",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.05",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.02",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'347.96",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.53",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.30",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.74",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.74",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.96"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.40",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.46",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.17",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.34",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.80",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.22"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.03"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.71",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.29",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.58",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.58 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.54"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.51",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.81",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.52",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.85",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.5% of the total average of 41.5 cycles between issuing two instructions.","global","56.54"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.06"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'439.35",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'095'978",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'539.13",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'148'661",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 521282 fused and 405260 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.972"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.21",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.81",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.41"
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'438.50",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'914'240",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'347.96",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'051'444",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'055.82",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'042'880",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'347.96",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'051'444",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'393.64",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'205'776",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","853'700",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.02",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","219.25",
"80","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 406406 excessive sectors (21% of the total 1897593 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.37"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'065",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.00",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.00",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.55",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.62",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.63",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'753.23",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.80",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.18",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.28",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.28",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.68"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.85",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.00",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.18",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.48",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.02",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.1"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.25"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.79",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.21",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.78",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.78 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.02",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.29",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.52",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.85",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.4% of the total average of 42.0 cycles between issuing two instructions.","global","57"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.18"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'476.95",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'115'832",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'561.84",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'160'649",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 522802 fused and 406514 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.007"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.46",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.97",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.16"
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'504.50",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'962'880",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'753.23",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'006'418",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","46'473.42",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'103'552",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'753.23",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'006'418",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'355.93",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'025'672",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","856'140",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.01",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","219.95",
"81","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 408966 excessive sectors (21% of the total 1904917 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.77"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'574",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.51",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.51",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.26",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.30",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.32",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'440.26",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.21",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.20",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.93",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.93",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.85"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.30",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.51",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.42",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.16",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.31",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.29"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.6"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.52",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.48",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.02",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.02 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.49"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.82",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.08",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.3% of the total average of 41.8 cycles between issuing two instructions.","global","56.49"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.37"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'563.43",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'161'492",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'647.66",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'205'966",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 526390 fused and 409330 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.988"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.40",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.94",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.21"
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'572.50",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'924'736",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'440.26",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'964'024",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'347.94",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'054'592",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'440.26",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'964'024",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'721.00",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'856'096",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","861'748",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.01",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.48",
"82","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 410261 excessive sectors (21% of the total 1915666 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.45"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","46'715",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","44.31",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","44.31",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","30.69",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.84",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.11",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","40'846.17",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.31",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.20",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.50",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.50",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.6"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.78",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.14",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","44.31",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.41",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.03",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.36",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.57"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.64"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.65",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.35",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.07",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.07 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","55.69"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.83",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.14",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.1% of the total average of 41.8 cycles between issuing two instructions.","global","55.69"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.41"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'580.84",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'170'684",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'681.43",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'223'793",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 527085 fused and 409917 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.02"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.07",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.72",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.55"
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'607",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'856'896",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","40'846.17",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","5'958'956",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'047.09",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","4'958'208",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","40'846.17",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","5'958'956",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","40'661.72",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","23'835'824",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","862'878",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.81",
"83","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 410801 excessive sectors (21% of the total 1918843 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.67"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'940",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.02",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.02",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.49",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.29",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.74",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'408.61",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.24",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.98",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.98",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.85"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.02",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.02",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.68",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.38",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.61",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.01"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.8"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.88",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.12",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.02",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.02 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.98"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.65",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.97",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.1% of the total average of 42.7 cycles between issuing two instructions.","global","56.98"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.94"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'555.08",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'157'082",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'656.87",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'210'828",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 526000 fused and 409090 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.988"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.10",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.74",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.52"
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'435.67",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'954'048",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'408.61",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'164'214",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'247.12",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'090'400",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'408.61",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'164'214",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'536.71",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'656'856",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","861'208",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.36",
"84","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 410016 excessive sectors (21% of the total 1914596 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.27"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'138",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.10",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.10",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.62",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.22",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.93",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'724.29",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.85",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.87",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.87",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.88"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.73",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.93",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.10",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.76",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.18",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.07",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.06"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.31"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.17",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.83",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.31",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.31 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.9"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.12",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.39",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.50",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.6% of the total average of 43.1 cycles between issuing two instructions.","global","56.9"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.21"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'630.92",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'197'126",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'714.15",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'241'071",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 529153 fused and 411555 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.985"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.15",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.78",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.47"
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'635.50",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'969'024",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'724.29",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'064'646",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'735.28",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'112'192",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'724.29",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'064'646",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'341.28",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'258'584",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","866'126",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","222.70",
"85","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 411653 excessive sectors (21% of the total 1924903 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.37"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'230",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.68",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.68",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.01",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.36",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.25",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'356.42",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.78",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.14",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.01",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.01",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.81"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.76",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.39",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.68",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.74",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.27",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.31",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.19"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.47"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.95",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.05",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.47",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.47 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.32"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.17",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.43",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.51",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.4% of the total average of 42.2 cycles between issuing two instructions.","global","56.32"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.74"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'568.49",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'164'162",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'653.17",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'208'875",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 526545 fused and 409535 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.993"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.98",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.67",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.64"
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'437.17",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'894'016",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'356.42",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'261'248",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","46'798.88",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'012'640",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'356.42",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'261'248",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","42'727.91",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","25'044'992",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","862'078",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","221.61",
"86","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 411210 excessive sectors (21% of the total 1917045 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.23"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","47'779",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.39",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.39",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.39",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.83",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.96",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","41'069.25",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.96",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.47",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.47",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.59"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.39",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.39",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.75",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.37",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.15",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.05"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.39"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.27",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.73",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.07",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.07 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.61"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.29",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.55",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.50",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.84",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.9% of the total average of 42.3 cycles between issuing two instructions.","global","56.61"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.26"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'660.90",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'212'954",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'746.01",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'257'894",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 530397 fused and 412531 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.021"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.37",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.92",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.24"
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'607.17",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'938'816",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","41'069.25",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'055'924",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'580.93",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'072'256",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","41'069.25",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'055'924",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'316.17",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'223'696",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","868'070",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","223.23",
"87","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 411565 excessive sectors (21% of the total 1928348 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.41"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'062",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.44",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.44",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.58",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.22",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.13",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","42'033.82",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.78",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.18",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.89",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.89",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.88"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.41",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.44",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.56",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.37",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.02",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.19"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.24"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.26",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.74",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.92",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.92 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.56"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.86",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.13",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.49",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.83",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.8% of the total average of 41.9 cycles between issuing two instructions.","global","56.56"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.19"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'736.19",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'252'710",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'823.66",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'298'891",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 533544 fused and 414966 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.987"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.17",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.79",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.45"
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'868",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'963'136",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","42'033.82",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'127'338",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","45'824",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'102'496",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","42'033.82",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'127'338",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","41'563.88",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'509'352",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","872'952",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","224.55",
"88","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 415480 excessive sectors (21% of the total 1941115 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.45"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","48'400",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.23",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.23",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","31.78",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.82",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.89",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","42'657.40",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.31",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.29",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.53",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.30",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.53",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.07"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.24",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.23",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.53",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.73",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.67",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.1"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.86"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.29",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.71",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.20",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.20 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.77"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.89",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.19",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.49",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.83",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 77.0% of the total average of 40.9 cycles between issuing two instructions.","global","56.77"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.98"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'771.78",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'271'502",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13'874.81",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'325'900",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 535022 fused and 416124 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.963"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.78",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.18",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.83"
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35'945.50",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","3'991'040",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","42'657.40",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'248'572",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","46'004.82",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'134'368",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","42'657.40",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'248'572",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","42'973.40",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","24'994'288",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","875'260",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","225.17",
"89","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 416286 excessive sectors (21% of the total 1946409 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.4"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","50'552",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.02",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.02",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","33.22",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.72",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","50.78",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","43'451.65",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","28.88",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.29",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.15",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.37",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.37",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.13"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.69",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.63",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.02",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.24",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.30",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.41",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.74"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.57"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.69",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.31",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.76",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.76 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.98"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.09",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.35",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.48",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.82",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.6% of the total average of 42.1 cycles between issuing two instructions.","global","57.98"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.8"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13'980.23",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'381'560",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'064.07",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'425'827",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 543708 fused and 422884 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.958"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.45",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.33",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.17"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","36'489.50",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'167'936",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","43'451.65",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'427'716",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","47'276.59",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'375'808",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","43'451.65",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'427'716",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","43'019.63",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","25'710'864",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.02% above the average, while the minimum instance value is 19.22% below the average.","global","5.368"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.34% above the average, while the minimum instance value is 20.27% below the average.","global","5.598"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.02% above the average, while the minimum instance value is 19.22% below the average.","global","5.368"
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","888'776",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","228.83",
"90","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 421593 excessive sectors (21% of the total 1976630 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.01"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","50'948",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","41.78",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","41.78",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","33.44",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.06",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","49.99",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","43'094.38",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.47",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.30",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.17",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.69",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.31",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.69",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.97"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.68",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","31.37",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","41.78",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.27",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.50",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.82",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","18.46"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.03"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.34",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.66",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.13",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.13 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","58.22"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.40",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.66",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.48",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.82",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.6% of the total average of 42.4 cycles between issuing two instructions.","global","58.22"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.5 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.06"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14'002.90",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'393'530",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'088.69",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'438'830",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 544653 fused and 423619 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.978"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.94",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.64",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.68"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","36'528.17",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'197'120",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","43'094.38",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'311'558",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","47'438.01",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'415'360",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","43'094.38",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'311'558",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","42'263.72",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","25'246'232",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.33% above the average, while the minimum instance value is 19.26% below the average.","global","5.709"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.50% above the average, while the minimum instance value is 19.63% below the average.","global","5.747"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.33% above the average, while the minimum instance value is 19.26% below the average.","global","5.709"
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","890'246",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","229.23",
"91","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 423857 excessive sectors (21% of the total 1981534 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","17.99"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","50'165",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.37",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.37",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","32.93",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.71",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","52.54",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","44'733.67",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.18",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.28",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.30",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.30",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.16"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.75",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.37",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.17",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.18",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.65",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.43"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.84"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.61",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.39",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.79",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.79 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.63"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.30",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.54",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.45",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.79",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.6% of the total average of 42.3 cycles between issuing two instructions.","global","56.63"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.96"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14'369.08",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'586'874",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'448.86",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'628'999",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 559917 fused and 435491 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.959"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.97",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.38",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.68"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","37'367.17",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'135'808",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","44'733.67",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'535'278",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","48'639.84",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'328'960",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","44'733.67",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'535'278",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","44'314.12",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'141'112",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 5.73% above the average, while the minimum instance value is 22.74% below the average.","global","5.129"
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","913'990",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","235.66",
"92","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 435618 excessive sectors (21% of the total 2036591 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.74"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","51'481",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.33",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.33",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","33.79",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.62",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.46",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","43'700.85",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.07",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.16",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.19",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.19",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.73"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.70",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","32.02",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.33",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.18",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","24.85",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","20.56",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.02"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","22.74"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.95",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.05",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.81",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.69",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.81 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.67"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.93",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.18",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.45",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.79",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 32.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 76.3% of the total average of 41.9 cycles between issuing two instructions.","global","57.67"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","12.91"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14'417.88",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","7'612'638",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14'504.64",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","7'658'451",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 561951 fused and 437073 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.013"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.82",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.93",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.82"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","37'429.67",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'244'352",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","43'700.85",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'585'466",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","49'778.61",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'469'120",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","43'700.85",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'585'466",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","44'025.39",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'341'864",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.51% above the average, while the minimum instance value is 21.90% below the average.","global","5.704"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.77% above the average, while the minimum instance value is 22.37% below the average.","global","5.97"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 6.51% above the average, while the minimum instance value is 21.90% below the average.","global","5.704"
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","917'154",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","236.51",
"93","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 436883 excessive sectors (21% of the total 2043639 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.68"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","51'994",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.60",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.60",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","34.14",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","33.70",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.54",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","47'350.66",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.13",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.28",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.20",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","32.22",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.29",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","32.22",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.2"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.24",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.60",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.31",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.68",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.36",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.79"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.64"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.25",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.75",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.93",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.93 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.4"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.89",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.13",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.40",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.75",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.3% of the total average of 41.9 cycles between issuing two instructions.","global","56.4"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.42"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'167.51",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'008'446",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'254.17",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'054'202",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 593199 fused and 461377 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.961"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.93",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.36",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.72"
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","38'920.67",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'285'312",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","47'350.66",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'682'646",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","50'520.31",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'524'704",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","47'350.66",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'682'646",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","45'874.85",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'730'584",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","965'762",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","249.66",
"94","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 463450 excessive sectors (21% of the total 2158923 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.84"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","52'029",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.69",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.69",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","34.08",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.81",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.85",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","45'928.50",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.11",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.32",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.20",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.28",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.33",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.28",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.7"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.63",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.69",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.26",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.30",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.32",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.88"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.62"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.08",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.92",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.47",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.47 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.31"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.70",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.96",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.40",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.75",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 73.7% of the total average of 40.7 cycles between issuing two instructions.","global","56.31"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.41"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'186.51",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'018'478",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'284.37",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'070'147",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 593991 fused and 461993 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.024"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.87",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.32",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.78"
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","38'938.50",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'278'272",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","45'928.50",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'701'468",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","50'866.22",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'510'304",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","45'928.50",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'701'468",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","46'199.80",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","26'805'872",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","966'994",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","250.00",
"95","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 460063 excessive sectors (21% of the total 2157722 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.9"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","54'044",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.23",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.23",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","35.52",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","35.61",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.26",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","46'997.55",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.29",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.35",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.20",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.94",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.36",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.94",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.37"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.72",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.23",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.36",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.72",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.49",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.66"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.84"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.76",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.24",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.57",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.57 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.77"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.18",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.44",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.35",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.71",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.8% of the total average of 40.2 cycles between issuing two instructions.","global","56.77"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.53"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'852.16",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'369'940",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'952.14",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'422'730",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 621738 fused and 483574 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.071"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.99",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.39",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.66"
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","40'164.17",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'460'032",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","46'997.55",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'952'036",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","52'579.50",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'751'264",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","46'997.55",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'952'036",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","47'251.66",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","27'808'144",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'010'156",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","261.67",
"96","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 482859 excessive sectors (21% of the total 2259071 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.76"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","54'322",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.92",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.92",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","35.68",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.71",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.20",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","48'055.97",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","29.99",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.31",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.19",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.03",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.32",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.03",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.8"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.72",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","33.28",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.92",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.32",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.56",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.29",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.64"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","23.64"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.38",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.33",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.62",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.60",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.60 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.08"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.75",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.97",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.36",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.72",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.1% of the total average of 40.8 cycles between issuing two instructions.","global","57.08"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 17.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.39"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","15'788.03",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","8'336'082",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15'873.13",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","8'381'014",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 619065 fused and 481495 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.016"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.45",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.69",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.2"
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","40'055.17",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'479'104",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","48'055.97",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","6'986'604",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","52'349.19",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","5'777'760",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","48'055.97",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","6'986'604",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","47'559.44",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","27'946'416",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'005'998",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","260.55",
"97","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 481171 excessive sectors (21% of the total 2250126 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.6"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","57'885",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.38",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.38",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","38.02",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","35.28",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","54.49",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","52'332.05",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","30.56",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.22",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","33.53",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.34",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","33.53",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.55"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.74",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.53",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.38",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.49",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.42",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","21.78",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","20.07"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.11"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","33.83",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","66.17",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.60",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.73",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.60 active warps per scheduler, but only an average of 0.73 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.62"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.19",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.39",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.27",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.64",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.7% of the total average of 40.2 cycles between issuing two instructions.","global","56.62"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.3 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","13.71"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17'464.78",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","9'221'406",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","17'548.86",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","9'265'799",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 688959 fused and 535857 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.061"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.68",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.48",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.94"
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","43'152.17",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'774'400",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","52'332.05",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'579'604",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","57'864.60",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'161'472",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","52'332.05",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'579'604",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","51'875.20",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","30'318'416",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'114'722",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","289.97",
"98","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 533650 excessive sectors (21% of the total 2501226 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.24"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","57'581",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","43.64",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","43.64",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","37.79",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","35.93",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","54.83",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","51'491.34",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.59",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.36",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.26",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.11",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.36",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.11",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.27"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.75",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.91",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","43.64",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.51",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.51",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.51",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","20.21"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.95"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","34.87",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.35",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","65.13",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.10",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.73",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.10 active warps per scheduler, but only an average of 0.73 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","56.36"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.43",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.63",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.27",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.64",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.3% of the total average of 40.4 cycles between issuing two instructions.","global","56.36"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.3 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14.17"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17'479.90",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","9'229'386",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","17'565.38",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","9'274'523",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 689589 fused and 536347 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.096"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.50",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.36",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.13"
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","43'171.83",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","4'748'288",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","51'491.34",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'339'730",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","56'870.45",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'116'640",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","51'491.34",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'339'730",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","50'371.54",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","29'358'920",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'115'702",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","290.23",
"99","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 536782 excessive sectors (21% of the total 2506653 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.11"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","62'385",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.54",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.54",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","40.96",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.86",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.69",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","54'608.93",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.18",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.39",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.24",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.95",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.40",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.95",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.86"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.71",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.67",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.54",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.53",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.06",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.28",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.79"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.66"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","34.46",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.34",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","65.54",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.88",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.88 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.46"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.27",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.45",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.19",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.57",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 75.6% of the total average of 40.3 cycles between issuing two instructions.","global","57.46"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.2 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14.06"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","18'999.47",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","10'031'718",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19'084.29",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","10'076'505",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 752931 fused and 585613 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.158"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.83",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.93",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.81"
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","45'597.83",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","5'144'448",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","54'608.93",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","8'078'904",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","61'807.98",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'638'976",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","54'608.93",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","8'078'904",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","55'383.54",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","32'315'616",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'214'234",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","316.89",
"100","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 581548 excessive sectors (21% of the total 2727686 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","19.05"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","62'532",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","42.46",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","42.46",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","41.02",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","36.46",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","53.71",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","55'170.82",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","31.57",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 2% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.38",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.26",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.62",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.38",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.62",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.02"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Tbyte/s","1.71",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","34.67",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","42.46",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.59",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.93",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","22.55",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","19.78"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","24.94"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","34.55",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.35",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","65.45",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.95",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.72",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.95 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","57.54"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.37",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.56",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","18.19",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","17.57",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 30.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 74.7% of the total average of 40.4 cycles between issuing two instructions.","global","57.54"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.2 threads being active per cycle. This is further reduced to 17.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","14.23"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","19'007.24",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","10'035'822",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19'098.69",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","10'084'106",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 753255 fused and 585865 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","2.137"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.40",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.29",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.23"
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","45'597.17",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","5'154'304",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","55'170.82",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","7'985'282",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","61'371.38",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","6'644'544",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","55'170.82",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","7'985'282",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","55'271.60",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","31'941'128",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.12",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","1'214'738",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","317.03",
"101","189053","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","13","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 582426 excessive sectors (21% of the total 2729435 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","18.92"
