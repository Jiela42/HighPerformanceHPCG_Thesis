AMGX,GH200,64x64x64,SymGS
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'842",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.99",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.99",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.73",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.36",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.31",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","10'487.97",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.96",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.16",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.89",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.89",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.78"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","883.97",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.18",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.99",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.97",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.25",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.69",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 80.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.318"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.39"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.23",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.77",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.03",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.84",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.03 active warps per scheduler, but only an average of 0.84 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.77"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.74",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.89",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.79",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.97",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 62.0% of the total average of 41.7 cycles between issuing two instructions.","global","61.98"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.8 threads being active per cycle. This is further reduced to 20.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.875"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3'050.28",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'610'548",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3'134.56",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'655'046",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 97380 fused and 75740 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.453"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 20.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","78.63",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","50.33",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (78.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","20.12"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'598.50",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'222'144",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","10'487.97",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'974'304",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'347.08",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'570'560",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","10'487.97",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'974'304",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","10'038.49",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'897'216",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.11% above the average, while the minimum instance value is 23.04% below the average.","global","9.191"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 17.62% above the average, while the minimum instance value is 21.50% below the average.","global","11.83"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.11% above the average, while the minimum instance value is 23.04% below the average.","global","9.191"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.10% above the average, while the minimum instance value is 7.46% below the average.","global","8.374"
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","183'804",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","40.98",
"0","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 72630 excessive sectors (21% of the total 346759 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.81"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'603",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.70",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.70",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.24",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.54",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.93",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","10'791.48",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.98",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.13",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.96",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.96",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.26"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","831",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.23",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.70",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.78",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.09",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.70",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 80.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.823"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.42"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.22",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.78",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.68",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.89",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.68 active warps per scheduler, but only an average of 0.89 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","67.78"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.46",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.67",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.80",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.99",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.4% of the total average of 42.5 cycles between issuing two instructions.","global","60.41"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.8 threads being active per cycle. This is further reduced to 20.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.878"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3'038.92",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'604'548",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3'125.04",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'650'022",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 96948 fused and 75404 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.406"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","78.85",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","50.46",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (78.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","19.9"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'540",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'284'608",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","10'791.48",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'965'724",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'507.50",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'652'928",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","10'791.48",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'965'724",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'699.52",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'862'896",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.48% above the average, while the minimum instance value is 22.74% below the average.","global","9.769"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 19.96% above the average, while the minimum instance value is 18.29% below the average.","global","13"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.48% above the average, while the minimum instance value is 22.74% below the average.","global","9.769"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 8.53% above the average, while the minimum instance value is 7.22% below the average.","global","6.197"
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","183'084",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","40.80",
"1","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 71753 excessive sectors (21% of the total 344784 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.12"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'097",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.98",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.98",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.95",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.79",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.60",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'075.48",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.51",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.23",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.55",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.26",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.55",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.8"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","799.23",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.89",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.98",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.21",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.08",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.06",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.874"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.67"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.06",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.94",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.96",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.82",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.96 active warps per scheduler, but only an average of 0.82 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.94"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.10",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.40",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.02",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.22",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.1% of the total average of 43.1 cycles between issuing two instructions.","global","61.07"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.0 threads being active per cycle. This is further reduced to 20.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.814"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'779.45",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'467'548",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'863.53",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'511'945",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 87084 fused and 67732 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.502"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.43",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.40",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.24"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'178.33",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'244'288",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'075.48",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'041'996",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'112.97",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'603'776",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'075.48",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'041'996",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'524.97",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","8'167'984",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.87% above the average, while the minimum instance value is 13.58% below the average.","global","13.42"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.98% above the average, while the minimum instance value is 16.34% below the average.","global","15.38"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.87% above the average, while the minimum instance value is 13.58% below the average.","global","13.42"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.45% above the average, while the minimum instance value is 7.99% below the average.","global","9.025"
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","166'644",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","36.65",
"2","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 64953 excessive sectors (21% of the total 310969 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.14"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'209",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.80",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.80",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.02",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.10",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.66",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'258.05",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.44",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.75",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.01",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.01",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.14"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","793.07",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.62",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.80",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.37",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.60",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.61",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.929"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.21"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.83",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.17",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.40",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.84",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.40 active warps per scheduler, but only an average of 0.84 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.17"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.46",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.96",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.02",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.22",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.1% of the total average of 43.5 cycles between issuing two instructions.","global","60.13"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.0 threads being active per cycle. This is further reduced to 20.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.157"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'775.19",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'465'298",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'871.08",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'515'930",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 86922 fused and 67606 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.469"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","81.76",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.33",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (81.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.94"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'171.50",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'253'888",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'258.05",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'949'168",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'095.99",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'614'816",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'258.05",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'949'168",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'312.39",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'796'672",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 21.75% above the average, while the minimum instance value is 15.05% below the average.","global","13.63"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.16% above the average, while the minimum instance value is 17.23% below the average.","global","13.98"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 21.75% above the average, while the minimum instance value is 15.05% below the average.","global","13.63"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.73% above the average, while the minimum instance value is 7.47% below the average.","global","7.716"
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","166'374",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","36.58",
"3","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 65870 excessive sectors (21% of the total 311233 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.22"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'733",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.66",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.66",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.37",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.87",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.95",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'413.08",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.87",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.15",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.73",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.58",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.58",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.76"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","746.54",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.14",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.66",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.34",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.24",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.28",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.271"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.85"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.21",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.79",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.42",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.42 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.79"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.01",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.32",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.09",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.30",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.7% of the total average of 43.0 cycles between issuing two instructions.","global","60.72"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.1 threads being active per cycle. This is further reduced to 20.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.898"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'702.03",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'426'673",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'784.61",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'470'275",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 84141 fused and 65443 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.399"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.70",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.57",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.97"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'039.17",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'296'384",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'413.08",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'948'254",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'914.05",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'673'952",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'413.08",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'948'254",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'921.02",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'793'016",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.95% above the average, while the minimum instance value is 12.03% below the average.","global","14.64"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.06% above the average, while the minimum instance value is 12.71% below the average.","global","15.15"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.95% above the average, while the minimum instance value is 12.03% below the average.","global","14.64"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 8.54% above the average, while the minimum instance value is 7.91% below the average.","global","5.833"
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","161'739",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","35.41",
"4","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 63112 excessive sectors (21% of the total 301065 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.32"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'571",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.76",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.76",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.24",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.20",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.22",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'229.08",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.26",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.16",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.71",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.01",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.01",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.55"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","752.65",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.55",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.76",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.45",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.07",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.88",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.377"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.41"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.16",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.84",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.04",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.04 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.84"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.25",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.61",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.11",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.32",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.3% of the total average of 43.2 cycles between issuing two instructions.","global","60.34"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.1 threads being active per cycle. This is further reduced to 20.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.667"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'684.51",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'417'423",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'769.39",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'462'240",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 83475 fused and 64925 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.416"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.51",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.44",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.17"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'017.67",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'284'096",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'229.08",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'002'070",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'601.38",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'652'256",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'229.08",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'002'070",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'182.53",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","8'008'280",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.08% above the average, while the minimum instance value is 11.95% below the average.","global","15.87"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.33% above the average, while the minimum instance value is 12.82% below the average.","global","15.34"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.08% above the average, while the minimum instance value is 11.95% below the average.","global","15.87"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.96% above the average, while the minimum instance value is 7.81% below the average.","global","7.386"
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","160'629",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","35.13",
"5","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 62970 excessive sectors (21% of the total 299002 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.2"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'947",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.02",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.02",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.82",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.24",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.48",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'603.55",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","17.93",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.69",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.25",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.25",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.87"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","764.35",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.21",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.02",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.13",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.91",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.65",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.506"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.15"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.46",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.54",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.16",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.83",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.16 active warps per scheduler, but only an average of 0.83 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.54"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.22",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.64",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.19",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.40",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.6% of the total average of 43.2 cycles between issuing two instructions.","global","61.6"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.2 threads being active per cycle. This is further reduced to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.498"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'602.36",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'374'048",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'688.28",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'419'413",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 80352 fused and 62496 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.462"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.02",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.41",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.63"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'888.67",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'233'920",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'603.55",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'978'896",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'893.39",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'585'632",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'603.55",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'978'896",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'825.63",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'915'584",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 30.25% above the average, while the minimum instance value is 8.63% below the average.","global","17.36"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.06% above the average, while the minimum instance value is 10.43% below the average.","global","15.93"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 30.25% above the average, while the minimum instance value is 8.63% below the average.","global","17.36"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.85% above the average, while the minimum instance value is 12.04% below the average.","global","5.65"
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","155'424",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","33.82",
"6","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 60866 excessive sectors (21% of the total 288259 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.2"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'083",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.83",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.83",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.92",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.62",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.34",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'114.70",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","17.83",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.69",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.46",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.46",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.77"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","754.63",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.11",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.83",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.10",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.92",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.59",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.438"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.08"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.16",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.84",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.63",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.92",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.63 active warps per scheduler, but only an average of 0.92 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.84"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.76",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.16",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.19",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.40",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.4% of the total average of 43.8 cycles between issuing two instructions.","global","61.35"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.2 threads being active per cycle. This is further reduced to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.463"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'601.89",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'373'798",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'685.41",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'417'897",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 80334 fused and 62482 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.379"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.41",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.66",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.24"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'873.67",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'242'112",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'114.70",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'987'810",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'004.10",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'600'896",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'114.70",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'987'810",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'617.86",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'951'240",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.76% above the average, while the minimum instance value is 7.69% below the average.","global","16.2"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 29.63% above the average, while the minimum instance value is 9.26% below the average.","global","16.95"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.76% above the average, while the minimum instance value is 7.69% below the average.","global","16.2"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.70% above the average, while the minimum instance value is 9.56% below the average.","global","8.424"
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","155'394",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","33.81",
"7","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 60365 excessive sectors (21% of the total 287631 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.11"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'412",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.04",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.04",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.47",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.37",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.15",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'568.14",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.69",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.17",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.41",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.22",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.41",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.27"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","765.76",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.83",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.04",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.04",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.83",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.11",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.381"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.62"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.79",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.21",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.78",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.78 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.21"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.34",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.88",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.28",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.50",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.2% of the total average of 43.3 cycles between issuing two instructions.","global","60.17"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.72"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'515.72",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'328'298",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'605.51",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'375'711",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 77058 fused and 59934 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.408"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.92",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.99",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.72"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'722.17",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'190'656",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'568.14",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'840'030",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","10'343.47",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'531'488",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'568.14",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'840'030",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'196.04",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'360'120",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.92% above the average, while the minimum instance value is 6.08% below the average.","global","15.32"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.27% above the average, while the minimum instance value is 6.71% below the average.","global","15.44"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.92% above the average, while the minimum instance value is 6.08% below the average.","global","15.32"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.35% above the average, while the minimum instance value is 8.42% below the average.","global","7.36"
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","149'934",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","32.43",
"8","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 58148 excessive sectors (21% of the total 276200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.65"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'761",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.76",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.76",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.70",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.05",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.78",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'408.86",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","17.76",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.69",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.08",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.08",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.89"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","754.53",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.03",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.76",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.08",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.88",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.53",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.22"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.02"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.08",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.92",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.54",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.87",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.54 active warps per scheduler, but only an average of 0.87 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.92"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.56",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.00",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.27",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.48",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 62.4% of the total average of 43.6 cycles between issuing two instructions.","global","62.36"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.391"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'529.92",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'335'798",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'613.79",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'380'083",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 77598 fused and 60354 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.444"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.25",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.20",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.38"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'763",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'218'432",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'408.86",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'942'762",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'072.93",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'568'544",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'408.86",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'942'762",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'409.71",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'771'048",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.17% above the average, while the minimum instance value is 6.19% below the average.","global","14.95"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 29.66% above the average, while the minimum instance value is 7.18% below the average.","global","16.95"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.17% above the average, while the minimum instance value is 6.19% below the average.","global","14.95"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.85% above the average, while the minimum instance value is 14.51% below the average.","global","6.676"
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","150'834",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","32.66",
"9","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 57953 excessive sectors (21% of the total 277685 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.14"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'943",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.04",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.04",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.86",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.21",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.14",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'167.88",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.30",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.71",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.34",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.34",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.72"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","722.88",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.47",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.04",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.43",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.57",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.88",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","7.953"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.35"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.46",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.54",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.83",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.88",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.83 active warps per scheduler, but only an average of 0.88 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.54"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.96",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.44",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.32",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.54",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.3% of the total average of 44.0 cycles between issuing two instructions.","global","61.28"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.553"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'475.94",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'307'298",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'559.69",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'351'514",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 75546 fused and 58758 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.448"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.33",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.89",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.29"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'638.50",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'234'176",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'167.88",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'846'320",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","9'336.40",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'588'320",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'167.88",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'846'320",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'135.40",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'385'280",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.64% above the average, while the minimum instance value is 3.41% below the average.","global","13.8"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.65% above the average, while the minimum instance value is 4.11% below the average.","global","16.08"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.64% above the average, while the minimum instance value is 3.41% below the average.","global","13.8"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 14.73% above the average, while the minimum instance value is 16.37% below the average.","global","8.311"
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","147'414",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.80",
"10","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56354 excessive sectors (21% of the total 270173 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.77"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'464",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.60",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.60",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.50",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.67",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.83",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'006.70",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.74",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.23",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.86",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.27",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.86",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.45"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","748.20",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.86",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.60",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.24",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.48",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.16",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.225"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.65"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.40",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.60",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.58",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.87",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.58 active warps per scheduler, but only an average of 0.87 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.6"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.42",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.02",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.34",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.56",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.4% of the total average of 46.4 cycles between issuing two instructions.","global","58.41"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.701"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'466.47",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'302'298",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'551.27",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'347'069",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 75186 fused and 58478 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.47"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.40",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.94",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.21"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'629.50",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'194'880",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'006.70",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'797'010",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'981.15",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'536'288",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'006.70",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'797'010",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'123.84",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'188'040",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.15% above the average, while the minimum instance value is 2.82% below the average.","global","14.79"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.40% above the average, while the minimum instance value is 6.23% below the average.","global","15.16"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.15% above the average, while the minimum instance value is 2.82% below the average.","global","14.79"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 15.97% above the average, while the minimum instance value is 13.98% below the average.","global","8.963"
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","146'814",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.64",
"11","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56412 excessive sectors (21% of the total 269416 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.75"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'283",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","23.56",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","23.56",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.46",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.13",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","28.31",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'100.93",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.95",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.89",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.30",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.30",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.74"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","943.18",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","20.63",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","23.56",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.02",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.38",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.88",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","10.11"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","15.47"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.30",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.70",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.07",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.91",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.07 active warps per scheduler, but only an average of 0.91 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","67.7"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.56",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.09",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.35",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.58",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.1% of the total average of 43.6 cycles between issuing two instructions.","global","61.07"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","8.194"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'449.90",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'293'548",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'535.57",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'338'782",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 74556 fused and 57988 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.44"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.62",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.08",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.99"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'578.33",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","932'608",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'100.93",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'458'090",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'022.57",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'193'184",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'100.93",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'458'090",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'850.28",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'832'360",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.91% above the average, while the minimum instance value is 9.03% below the average.","global","7.686"
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","145'764",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.38",
"12","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56226 excessive sectors (21% of the total 267440 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.57"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'467",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","23.06",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","23.06",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.55",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.79",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","27.88",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'446.14",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.93",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.15",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.89",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.81",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.81",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.49"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","924.92",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","20.61",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","23.06",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.87",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.48",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.89",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.987"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","15.46"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","32.14",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","67.86",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.82",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.95",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.82 active warps per scheduler, but only an average of 0.95 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","67.86"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.11",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.71",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.37",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.60",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 56.3% of the total average of 46.1 cycles between issuing two instructions.","global","56.32"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","8.171"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'433.57",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'284'923",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'517.71",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'329'350",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 73935 fused and 57505 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.37"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.68",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","56.12",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","10.92"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'547.50",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","946'688",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'446.14",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'449'096",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'909.18",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'212'192",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'446.14",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'449'096",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'834.61",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'796'384",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.06% above the average, while the minimum instance value is 8.61% below the average.","global","7.555"
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","144'729",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.12",
"13","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56048 excessive sectors (21% of the total 265575 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.22"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'009",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.67",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.67",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.94",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.98",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.43",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'951.82",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.13",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.85",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.20",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.20",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.75"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","865.48",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.85",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.67",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.95",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.19",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.38",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.457"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.88"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.73",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.27",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.72",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.72 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.27"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.65",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.24",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.42",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.65",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.6% of the total average of 44.7 cycles between issuing two instructions.","global","60.56"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.849"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'395.45",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'264'798",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'480.74",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'309'832",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 72486 fused and 56378 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.427"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.81",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.92",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.83"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'471.67",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","990'464",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'951.82",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'479'922",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'901.75",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'270'944",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'951.82",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'479'922",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'072.20",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'919'688",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.62% above the average, while the minimum instance value is 9.97% below the average.","global","11.11"
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","142'314",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","30.51",
"14","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 55038 excessive sectors (21% of the total 260607 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.61"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'997",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.60",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.60",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.90",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.91",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.17",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'226.23",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.16",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.16",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.86",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.01",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.01",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.35"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","865.20",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.88",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.60",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.95",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.26",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.47",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.359"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.91"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.84",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.16",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.57",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.93",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.57 active warps per scheduler, but only an average of 0.93 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.16"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.02",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.57",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.42",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.65",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.3% of the total average of 44.0 cycles between issuing two instructions.","global","61.28"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.859"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'384.80",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'259'173",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'468.77",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'303'508",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 72081 fused and 56063 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.371"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.21",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.17",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.42"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'452.17",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","989'440",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'226.23",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'470'242",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'879.90",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'272'096",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'226.23",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'470'242",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'006.00",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'880'968",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.96% above the average, while the minimum instance value is 9.42% below the average.","global","8.896"
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","141'639",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","30.34",
"15","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 54418 excessive sectors (21% of the total 258572 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.52"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'959",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.50",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.50",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.90",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.02",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.79",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'098.44",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.41",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.17",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.87",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.17",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.21",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.17",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.25"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","858.66",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","20.07",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.50",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.47",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.45",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.67",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.283"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","15.05"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.35",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.65",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.87",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.88",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.87 active warps per scheduler, but only an average of 0.88 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.65"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.24",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.81",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.45",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.68",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.6% of the total average of 44.2 cycles between issuing two instructions.","global","58.63"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.925"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'359.70",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'245'923",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'443.29",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'290'059",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 71127 fused and 55321 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.375"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.69",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.20",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.97"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'418.50",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","986'624",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'098.44",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'439'338",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'036.65",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'266'912",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'098.44",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'439'338",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'794.78",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'757'352",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.71% above the average, while the minimum instance value is 8.81% below the average.","global","8.346"
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","140'049",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.94",
"16","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 53693 excessive sectors (21% of the total 255106 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.82"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'712",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.71",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.71",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.71",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.34",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.63",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'251.82",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.75",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.43",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.43",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.63"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","873.23",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.46",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.71",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.48",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.44",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.27",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.586"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.59"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.96",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.04",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.68",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.89",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.68 active warps per scheduler, but only an average of 0.89 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","70.04"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","45.65",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.30",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.47",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.70",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 56.8% of the total average of 45.6 cycles between issuing two instructions.","global","56.84"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.678"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'343.84",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'237'548",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'428.80",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'282'406",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 70524 fused and 54852 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.338"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.41",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.02",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.25"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'384.33",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","969'344",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'251.82",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'474'198",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'878.03",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'239'360",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'251.82",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'474'198",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'105.63",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'896'792",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.32% above the average, while the minimum instance value is 8.70% below the average.","global","7.518"
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","139'044",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.68",
"17","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 53229 excessive sectors (21% of the total 252910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.84"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'897",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.34",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.34",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.84",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.29",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.06",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'852.33",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.71",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.54",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.22",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.54",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.01"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","856.13",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.40",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.34",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.93",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.36",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.30",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.468"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.55"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.08",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.92",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.76",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.84",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.76 active warps per scheduler, but only an average of 0.84 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.92"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","45.77",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.37",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.49",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.73",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 57.8% of the total average of 45.8 cycles between issuing two instructions.","global","57.84"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.647"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'316.62",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'223'173",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'398.00",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'266'145",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 69489 fused and 54047 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.385"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.49",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.43",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.19"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'369.83",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","983'040",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'852.33",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'457'874",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'751.56",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'260'288",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'852.33",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'457'874",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'973.03",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'831'496",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 15.21% above the average, while the minimum instance value is 8.53% below the average.","global","8.981"
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","137'319",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.25",
"18","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 52646 excessive sectors (21% of the total 249192 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.47"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'832",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.57",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.57",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.81",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.76",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.32",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'773.81",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.85",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.04",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.04",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.77"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","864.69",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.54",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.57",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.98",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.24",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.36",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.513"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.66"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.33",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.67",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.38",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.38 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.67"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.10",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.68",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.49",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.73",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.1% of the total average of 44.1 cycles between issuing two instructions.","global","60.1"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.697"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'329.40",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'229'923",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'412.84",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'273'982",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 69975 fused and 54425 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.409"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.01",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.76",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.66"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'395.50",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","978'304",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'773.81",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'457'866",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'220.62",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'254'240",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'773.81",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'457'866",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'955.45",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'831'464",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.58% above the average, while the minimum instance value is 9.17% below the average.","global","7.918"
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","138'129",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.45",
"19","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 52506 excessive sectors (21% of the total 250781 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.17"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'688",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.76",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.76",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.71",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.09",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.03",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'898.52",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.94",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.17",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.85",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.31",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.21",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.31",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.14"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","870.90",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.61",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.76",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.64",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.06",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.43",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.438"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.71"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.10",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.90",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.62",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.87",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.62 active warps per scheduler, but only an average of 0.87 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.9"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.80",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.36",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.51",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.75",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.8% of the total average of 43.8 cycles between issuing two instructions.","global","58.81"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.714"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'311.17",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'220'298",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'393.81",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'263'933",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 69282 fused and 53886 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.373"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.78",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.62",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.89"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'372.67",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","964'608",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'898.52",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'440'186",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'871.90",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'236'672",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'898.52",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'440'186",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'697.81",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'760'744",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.06% above the average, while the minimum instance value is 8.12% below the average.","global","8.593"
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","136'974",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.16",
"20","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 51850 excessive sectors (21% of the total 248203 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.77"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'605",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.83",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.83",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.68",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.07",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.33",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'171.28",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.81",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.13",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.21",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.17",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.21",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.72"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","872.37",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.47",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.83",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.55",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.17",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.39",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.569"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.6"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.59",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.41",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.41",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.88",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.41 active warps per scheduler, but only an average of 0.88 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.41"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","47.09",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.83",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.53",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.77",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 56.1% of the total average of 47.1 cycles between issuing two instructions.","global","56.09"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.655"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'302.41",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'215'673",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'387.10",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'260'389",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 68949 fused and 53627 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.321"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.21",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.25",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.47"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'361.83",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","959'104",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'171.28",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'444'646",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'963.66",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'229'088",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'171.28",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'444'646",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'802.63",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'778'584",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.87% above the average, while the minimum instance value is 9.19% below the average.","global","11.74"
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","136'419",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.02",
"21","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 52075 excessive sectors (21% of the total 247741 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.07"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'012",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.07",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.07",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.87",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.30",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.87",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'762.79",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.05",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.81",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.63",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.63",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.98"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","847.19",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.76",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.07",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.53",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.98",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.86",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.393"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.07"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.21",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.79",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.74",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.82",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.74 active warps per scheduler, but only an average of 0.82 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","70.79"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.61",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.25",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.55",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.79",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.2% of the total average of 43.6 cycles between issuing two instructions.","global","60.21"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.379"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'291.52",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'209'923",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'377.91",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'255'535",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 68535 fused and 53305 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.382"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.69",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.92",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'341.83",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","988'928",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'762.79",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'490'794",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'826.28",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'266'816",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'762.79",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'490'794",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'140.10",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'963'176",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 19.66% above the average, while the minimum instance value is 9.04% below the average.","global","11.66"
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","135'729",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","28.84",
"22","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 51513 excessive sectors (21% of the total 246038 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.42"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'107",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.81",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.81",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.49",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.21",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'690.32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.23",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.19",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.80",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.80",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.87"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","833.73",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.95",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.81",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.55",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.88",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.03",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.157"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.21"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.34",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.66",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.24",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.24 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.66"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.64",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.26",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.57",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.80",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.1% of the total average of 43.6 cycles between issuing two instructions.","global","59.08"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.6 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.427"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'283.71",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'205'798",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'368.32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'250'473",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 68238 fused and 53074 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.389"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.53",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.82",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.16"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'342.33",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'001'472",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'690.32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'472'650",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'663.04",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'283'328",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'690.32",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'472'650",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'806.93",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'890'600",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.66% above the average, while the minimum instance value is 7.90% below the average.","global","6.108"
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","135'234",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","28.72",
"23","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 51731 excessive sectors (21% of the total 245683 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.07"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'827",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.03",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.03",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.78",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.38",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.30",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'908.44",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.67",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.55",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.55",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.5"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","845.63",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.35",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.03",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.61",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.96",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.40",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.183"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.51"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.50",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.50",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.97",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.97 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.5"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.54",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.12",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.61",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.85",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.2% of the total average of 42.5 cycles between issuing two instructions.","global","59.23"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.6 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.552"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'253.64",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'189'923",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'337.31",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'234'099",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 67095 fused and 52185 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.328"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","81.74",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.31",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (81.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.96"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'281",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","976'896",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'908.44",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'423'556",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'635.22",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'251'456",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'908.44",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'423'556",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'663.84",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'694'224",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.16% above the average, while the minimum instance value is 7.22% below the average.","global","5.953"
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","133'329",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","28.24",
"24","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 50969 excessive sectors (21% of the total 241799 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.35"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'911",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.53",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.53",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.87",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.21",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.65",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'094.42",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.34",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.33",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.13",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.33",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.11"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","820.52",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.99",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.53",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.39",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.04",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.26",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.962"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.24"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.03",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.97",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.42",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.87",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.42 active warps per scheduler, but only an average of 0.87 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.97"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.68",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.39",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.68",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.92",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 24.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 55.1% of the total average of 44.7 cycles between issuing two instructions.","global","55.13"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.7 threads being active per cycle. This is further reduced to 20.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.386"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'208.66",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'166'173",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'293.00",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'210'705",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 65385 fused and 50855 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.264"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","80.64",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","51.61",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (80.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","18.08"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'205.17",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","983'040",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'094.42",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'418'522",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'670.94",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'261'824",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'094.42",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'418'522",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'635.20",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'674'088",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.65% above the average, while the minimum instance value is 8.81% below the average.","global","10.88"
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","130'479",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","27.52",
"25","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 49832 excessive sectors (21% of the total 236107 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.32"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'854",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.05",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.05",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.84",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.01",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.70",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'615.27",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.14",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.13",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.78",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.23",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.17",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.23",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.57"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","800.07",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.92",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.05",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.43",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.98",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.56",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.971"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.44"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.01",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.99",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.14",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.75",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.14 active warps per scheduler, but only an average of 0.75 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.99"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.36",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.04",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.78",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.03",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 24.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 57.5% of the total average of 43.4 cycles between issuing two instructions.","global","57.47"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.8 threads being active per cycle. This is further reduced to 21.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.903"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'142.85",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'131'423",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'226.01",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'175'334",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 62883 fused and 48909 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.292"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 21.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","77.51",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","49.60",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (77.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","21.26"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'083.67",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","977'536",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'615.27",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'459'306",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'605.39",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'257'408",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'615.27",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'459'306",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'948.37",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'837'224",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.90% above the average, while the minimum instance value is 8.07% below the average.","global","8.652"
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","126'309",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","26.47",
"26","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 48071 excessive sectors (21% of the total 227518 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.27"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'383",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.80",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.80",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.49",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.06",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.23",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'471.11",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.01",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.77",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.31",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.13",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.31",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.96"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","795.01",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.71",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.80",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.33",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.69",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.73",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.825"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.28"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.39",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.61",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","11.94",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.74",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 11.94 active warps per scheduler, but only an average of 0.74 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.61"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.04",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.73",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.96",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.22",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 24.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 57.2% of the total average of 42.0 cycles between issuing two instructions.","global","57.17"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.0 threads being active per cycle. This is further reduced to 21.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.74"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'033.23",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'073'548",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'114.77",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'116'600",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 58716 fused and 45668 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.23"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 24.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","74.12",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","47.44",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (74.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","24.7"
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'875.67",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","939'520",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'471.11",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'395'200",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'453.46",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'203'456",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'471.11",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'395'200",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'449.21",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'580'800",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","119'364",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","24.71",
"27","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 45634 excessive sectors (21% of the total 213648 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.7"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'317",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.67",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.67",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.46",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.73",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.88",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'361.51",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.29",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.74",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.02",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.08",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.02",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.51"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","746.95",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.95",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.67",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.13",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.87",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.59",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.34"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.71"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.17",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.83",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","11.14",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.71",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 11.14 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.83"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.54",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.18",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.17",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.45",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 22.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 55.8% of the total average of 39.5 cycles between issuing two instructions.","global","55.81"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.2 threads being active per cycle. This is further reduced to 21.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.362"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'910.13",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'008'548",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'988.91",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'050'143",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 54036 fused and 42028 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.149"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","69.63",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","44.56",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (69.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","29.27"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'625.83",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","932'096",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'361.51",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'360'906",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'216.57",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'200'960",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'361.51",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'360'906",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'061.25",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'443'624",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.43% above the average, while the minimum instance value is 8.36% below the average.","global","5.44"
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","111'564",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","22.74",
"28","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 41934 excessive sectors (21% of the total 196723 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.3"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'062",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.55",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","17.55",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.26",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.82",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","21.25",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6'858.48",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.35",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.03",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.70",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.83",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.07",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.83",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.45"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","704.78",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.29",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","17.55",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.83",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.66",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.33",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","7.782"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.22"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.80",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.20",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.73",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.72",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.73 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.2"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.61",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.32",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.47",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.76",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 20.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 52.8% of the total average of 38.6 cycles between issuing two instructions.","global","52.82"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.5 threads being active per cycle. This is further reduced to 21.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","5.873"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'761.93",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","930'298",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'840.01",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","971'527",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 48402 fused and 37646 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.105"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 32.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","66.32",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","42.45",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (66.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","32.62"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'333",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","911'360",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6'858.48",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'323'408",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6'614.15",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'172'640",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6'858.48",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'323'408",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","6'618.47",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'293'632",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.48% above the average, while the minimum instance value is 4.29% below the average.","global","7.854"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 7.58% above the average, while the minimum instance value is 5.33% below the average.","global","5.001"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.48% above the average, while the minimum instance value is 4.29% below the average.","global","7.854"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.18% above the average, while the minimum instance value is 10.16% below the average.","global","7.135"
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","102'174",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","20.37",
"29","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 37913 excessive sectors (21% of the total 176746 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.61"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'926",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.55",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","15.55",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.20",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.09",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","19.40",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6'289.02",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","16.42",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.00",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.63",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.19",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.05",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.19",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.53"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","623.57",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","14.48",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","15.55",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.72",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.51",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.46",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","7.102"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","10.86"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.19",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.81",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.86",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.63",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.86 active warps per scheduler, but only an average of 0.63 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.81"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","37.67",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.44",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.91",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","22.24",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 19.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 50.5% of the total average of 37.7 cycles between issuing two instructions.","global","50.52"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.9 threads being active per cycle. This is further reduced to 22.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","5.008"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'572.53",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","830'298",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'646.81",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","869'516",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 41202 fused and 32046 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.025"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.34",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.90",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.67"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'923",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","902'144",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6'289.02",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'323'976",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6'639.15",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'160'160",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6'289.02",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'323'976",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","6'287.98",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'295'904",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.47% above the average, while the minimum instance value is 10.69% below the average.","global","8.446"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.25% above the average, while the minimum instance value is 9.46% below the average.","global","7.681"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.47% above the average, while the minimum instance value is 10.69% below the average.","global","8.446"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 15.96% above the average, while the minimum instance value is 10.79% below the average.","global","8.768"
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","90'174",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","17.34",
"30","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 32186 excessive sectors (21% of the total 150484 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.75"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'994",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.03",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","13.03",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.23",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","20.71",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","16.34",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5'957.41",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","14.90",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.91",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.57",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.03",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.03",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.36"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","520.88",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","12.84",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","13.03",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.86",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.90",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.06",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","6.07"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","9.631"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.93",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.07",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.63",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.53",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.63 active warps per scheduler, but only an average of 0.53 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.07"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.08",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.93",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","23.56",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","22.93",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 17.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 48.5% of the total average of 36.1 cycles between issuing two instructions.","global","48.5"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 23.6 threads being active per cycle. This is further reduced to 22.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","4.224"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'361.83",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","719'048",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'431.59",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","755'878",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 33192 fused and 25816 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.872"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.7%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","54.39",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","34.81",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (54.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.75"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'452.50",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","903'680",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5'957.41",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'268'368",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'907.71",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'167'840",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5'957.41",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'268'368",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5'983.05",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'073'472",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.31% above the average, while the minimum instance value is 9.14% below the average.","global","6.39"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.98% above the average, while the minimum instance value is 10.18% below the average.","global","7.456"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.31% above the average, while the minimum instance value is 9.14% below the average.","global","6.39"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.57% above the average, while the minimum instance value is 11.35% below the average.","global","8.533"
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","76'824",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","13.97",
"31","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 26438 excessive sectors (22% of the total 121871 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.54"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'417",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","11.47",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","11.22",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.85",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.02",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.53",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5'827.33",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","13.61",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.81",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.52",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","21.37",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.85",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","21.37",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","87.51"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","449.50",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","11.47",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","11.22",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.72",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.03",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.75",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","5.384"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","8.604"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.22",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.78",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.61",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.48",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.61 active warps per scheduler, but only an average of 0.48 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","77.78"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.27",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.20",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","24.30",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","23.72",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 14.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 41.9% of the total average of 34.3 cycles between issuing two instructions.","global","41.94"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 24.3 threads being active per cycle. This is further reduced to 23.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","3.52"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'179.07",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","622'548",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'245.44",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","657'594",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 26244 fused and 20412 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.7049"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 51.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","47.99",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","30.71",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (48.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","51.25"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'004",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","856'960",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5'827.33",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'208'254",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'798.14",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'110'912",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5'827.33",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'208'254",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5'604.99",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'833'016",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.41% above the average, while the minimum instance value is 55.64% below the average.","global","8.539"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.86% above the average, while the minimum instance value is 55.63% below the average.","global","7.873"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.41% above the average, while the minimum instance value is 55.64% below the average.","global","8.539"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.27% above the average, while the minimum instance value is 11.59% below the average.","global","7.148"
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","65'244",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","11.05",
"32","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 20695 excessive sectors (22% of the total 96251 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.77"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'129",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.35",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","8.53",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.66",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.95",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.84",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","4'532.83",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","11.78",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.86",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.45",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.61",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.90",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.61",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.08"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","340.50",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","9.35",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","8.53",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.97",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.60",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.08",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","4.071"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","7.011"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.32",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.68",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.95",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.49",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.95 active warps per scheduler, but only an average of 0.49 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.68"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.08",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.99",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","25.47",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.98",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 11.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 34.7% of the total average of 34.1 cycles between issuing two instructions.","global","34.68"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","970.50",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","512'423",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'025.06",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","541'233",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 18315 fused and 14245 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.6324"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 49.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","49.32",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","31.57",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (49.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","49.9"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","1'475.50",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","830'464",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","4'532.83",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'149'092",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'354.44",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'078'368",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","4'532.83",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'149'092",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","4'396.29",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'596'368",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 28.27% above the average, while the minimum instance value is 54.55% below the average.","global","14.72"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 26.87% above the average, while the minimum instance value is 58.72% below the average.","global","13.57"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 28.27% above the average, while the minimum instance value is 54.55% below the average.","global","14.72"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.45% above the average, while the minimum instance value is 14.50% below the average.","global","8.316"
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","52'029",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","7.71",
"33","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 14308 excessive sectors (21% of the total 67058 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.17"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'778",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.65",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","6.29",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.43",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.38",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","8.50",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'791.87",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","10.37",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.87",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.38",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.82",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.91",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.82",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.19"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","251.78",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.65",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.85",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.82",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.28",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.37",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.351"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.74"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.38",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.62",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.87",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.49",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.87 active warps per scheduler, but only an average of 0.49 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.62"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.66",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.48",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.68",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.28",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","820.88",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","433'423",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","865.22",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","456'838",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 12627 fused and 9821 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5212"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 50.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","49.17",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","31.47",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (49.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","50.05"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","1'054.33",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","804'352",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'791.87",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'136'648",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'984.09",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'041'216",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'791.87",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'136'648",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'700.38",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'546'592",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 37.10% above the average, while the minimum instance value is 52.71% below the average.","global","16.34"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 37.76% above the average, while the minimum instance value is 54.19% below the average.","global","16.23"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 37.10% above the average, while the minimum instance value is 52.71% below the average.","global","16.34"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.04% above the average, while the minimum instance value is 14.37% below the average.","global","10.13"
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","42'549",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","5.31",
"34","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 10035 excessive sectors (22% of the total 46393 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.94"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'620",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.41",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.28",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.34",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.10",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.69",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'196.52",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.71",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.88",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.38",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.94",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.38",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.12"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","171.03",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.41",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.84",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.89",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.00",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.71",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.778"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.804"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.84",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.16",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.42",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.51",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.42 active warps per scheduler, but only an average of 0.51 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.16"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.90",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.93",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.96",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.66",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","705.11",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","372'298",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","747.35",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","394'601",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 8226 fused and 6398 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4028"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.74",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","33.12",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.44"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","705.50",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","791'296",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'196.52",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'126'348",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'685.83",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'027'776",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'196.52",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'126'348",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'008.36",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'505'392",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.02% above the average, while the minimum instance value is 45.16% below the average.","global","18.74"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 48.99% above the average, while the minimum instance value is 45.85% below the average.","global","17.27"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 50.02% above the average, while the minimum instance value is 45.16% below the average.","global","18.74"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 21.25% above the average, while the minimum instance value is 22.28% below the average.","global","9.299"
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","35'214",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.46",
"35","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6339 excessive sectors (21% of the total 30043 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.235"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'744",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.33",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.77",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.40",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.88",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.01",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'601.15",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.02",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.29",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.54",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.02",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.54",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.89"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","110.84",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.33",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.99",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.89",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","30.35",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.02",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.335"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.999"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.77",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.23",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.69",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.69 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.23"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.74",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.79",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.11",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.90",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","626.27",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","330'673",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","664.36",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","350'783",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 5229 fused and 4067 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.3146"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 44.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.05",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.23",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","44.08"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","461.83",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","801'280",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'601.15",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'151'408",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'478.52",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'039'296",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'601.15",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'151'408",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'578.45",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'605'632",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.76% above the average, while the minimum instance value is 35.99% below the average.","global","16.33"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 56.66% above the average, while the minimum instance value is 41.71% below the average.","global","16.75"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 54.76% above the average, while the minimum instance value is 35.99% below the average.","global","16.33"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.30% above the average, while the minimum instance value is 20.44% below the average.","global","10.88"
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","30'219",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.20",
"36","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4178 excessive sectors (22% of the total 19254 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.977"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'079",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.89",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.80",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.98",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.89",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.49",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'407.87",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.10",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.99",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.99",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.53"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","71.53",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.89",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.69",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.38",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","32.92",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.10",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.117"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.664"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.53",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.47",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.45",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.61",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.45 active warps per scheduler, but only an average of 0.61 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.47"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.38",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.61",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.14",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.00",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","569.46",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","300'673",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","601.74",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","317'717",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 3069 fused and 2387 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1995"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 39.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","59.98",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","38.39",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (60.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","39.06"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","278.67",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","744'448",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'407.87",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'098'816",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'128.11",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","969'024",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'407.87",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'098'816",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'267.93",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'395'264",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.96% above the average, while the minimum instance value is 28.86% below the average.","global","17.34"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 60.44% above the average, while the minimum instance value is 29.72% below the average.","global","16.47"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 59.96% above the average, while the minimum instance value is 28.86% below the average.","global","17.34"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.68% above the average, while the minimum instance value is 17.73% below the average.","global","9.275"
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","26'619",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.29",
"37","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2359 excessive sectors (21% of the total 11200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.614"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'219",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.42",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.99",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.08",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.06",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.44",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'292.44",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.85",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.62",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.62",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.38"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","39.33",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.42",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.32",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.87",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","37.78",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.85",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.5 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.006"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.316"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.78",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.22",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","11.72",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 11.72 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.22"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.73",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.30",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.95",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.88",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","530.87",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","280'298",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","564.34",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","297'973",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1602 fused and 1246 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1094"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 33.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","65.41",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.86",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (65.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","33.55"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","155.67",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","755'968",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'292.44",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'099'452",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'593.03",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","985'728",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'292.44",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'099'452",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'961.06",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'397'808",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.00% above the average, while the minimum instance value is 21.39% below the average.","global","17.06"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.84% above the average, while the minimum instance value is 29.88% below the average.","global","15.5"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.00% above the average, while the minimum instance value is 21.39% below the average.","global","17.06"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 26.28% above the average, while the minimum instance value is 27.22% below the average.","global","9.197"
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","24'174",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.67",
"38","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1434 excessive sectors (24% of the total 6042 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.305"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'083",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.29",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.55",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.98",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.10",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.76",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'168.95",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.93",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.09",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.09",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.73"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","22.12",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.29",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.23",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.01",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","43.81",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.93",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.893"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.215"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.75",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.25",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.09",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.09 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.25"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.90",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.05",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.41",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.37",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","511.69",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","270'173",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","544.14",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","287'304",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 873 fused and 679 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.063"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 31.8%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","67.14",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","42.97",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (67.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","31.79"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","86.17",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","747'776",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'168.95",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'075'060",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'891.61",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","971'136",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'168.95",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'075'060",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'829.13",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'300'240",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.51% above the average, while the minimum instance value is 18.95% below the average.","global","16.65"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.28% above the average, while the minimum instance value is 21.00% below the average.","global","15.11"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.51% above the average, while the minimum instance value is 18.95% below the average.","global","16.65"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 35.68% above the average, while the minimum instance value is 50.58% below the average.","global","10.2"
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'959",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.37",
"39","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 723 excessive sectors (22% of the total 3245 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.369"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'211",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.32",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.34",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.05",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.90",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.48",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'894.14",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.21",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.06",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.12",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.12",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.12",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","13.63",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.32",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.29",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.88",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","52.71",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.21",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.3 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.982"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.242"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.35",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.65",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.17",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.17 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.65"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.51",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.58",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.63",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","501.75",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","264'923",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","532.65",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","281'237",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 495 fused and 385 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0409"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.80",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.19",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.21"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","53.67",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","756'736",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'894.14",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'035'244",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'472.44",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","986'208",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'894.14",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'035'244",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'755.18",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'140'976",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.80% above the average, while the minimum instance value is 22.18% below the average.","global","16.38"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.52% above the average, while the minimum instance value is 16.59% below the average.","global","15.11"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.80% above the average, while the minimum instance value is 22.18% below the average.","global","16.38"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 40.47% above the average, while the minimum instance value is 60.04% below the average.","global","9.739"
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'329",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.21",
"40","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 466 excessive sectors (25% of the total 1896 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","5.915"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'739",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.21",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.23",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.73",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.19",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.64",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'835.85",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.08",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.72",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.15",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.72",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","9.43",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.21",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.19",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.49",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","58.40",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.08",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.898"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.158"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.01",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.99",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.31",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.31 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","68.99"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.25",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.30",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.79",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.77",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","496.78",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","262'298",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","527.33",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","278'432",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 306 fused and 238 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.02609"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.01",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.33",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.99"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","35.17",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","718'848",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'835.85",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'046'980",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'140.58",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","934'368",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'835.85",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'046'980",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'700.39",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'187'920",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.05% above the average, while the minimum instance value is 11.49% below the average.","global","15.98"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.18% above the average, while the minimum instance value is 18.61% below the average.","global","14.83"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.05% above the average, while the minimum instance value is 11.49% below the average.","global","15.98"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.69% above the average, while the minimum instance value is 68.00% below the average.","global","9.389"
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'014",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.13",
"41","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 267 excessive sectors (23% of the total 1146 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","5.124"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'035",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.12",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.11",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.92",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.38",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.33",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'790.48",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.97",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.10",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.26",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.17",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.26",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","4.45",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.12",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.11",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.18",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","60.16",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.97",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.6 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.722"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.088"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.75",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.25",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.86",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.86 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.25"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.16",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.31",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.92",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.91",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 40.2% of the total average of 33.2 cycles between issuing two instructions.","global","40.16"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","491.80",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'673",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","523.82",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","276'577",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 117 fused and 91 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.01023"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.81",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.48",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.17"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","17.17",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","740'736",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'790.48",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'055'108",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'286.41",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","968'160",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'790.48",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'055'108",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'760.86",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'220'432",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.87% above the average, while the minimum instance value is 12.48% below the average.","global","14.98"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.53% above the average, while the minimum instance value is 17.88% below the average.","global","15.1"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.87% above the average, while the minimum instance value is 12.48% below the average.","global","14.98"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 56.93% above the average, while the minimum instance value is 83.13% below the average.","global","7.262"
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'699",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.05",
"42","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 80 excessive sectors (19% of the total 418 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","2.441"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'739",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.26",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.08",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.76",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.51",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.34",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'768.01",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.33",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.11",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.75",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.75",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.1%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","3.38",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.26",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.26",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.58",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","57.65",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.33",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.861"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.197"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.92",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.08",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.51",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.51 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.08"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.99",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.47",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.96",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.96",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.4 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 30.7% of the total average of 34.0 cycles between issuing two instructions.","global","30.72"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","490.15",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'798",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","526",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","277'728",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 54 fused and 42 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.00478"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.12",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.04",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.86"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","12.67",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","718'592",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'768.01",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'013'392",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","781.04",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","936'384",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'768.01",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'013'392",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'701.16",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'053'568",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.64% above the average, while the minimum instance value is 12.73% below the average.","global","15.35"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.93% above the average, while the minimum instance value is 15.41% below the average.","global","15.05"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.64% above the average, while the minimum instance value is 12.73% below the average.","global","15.35"
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'594",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.02",
"43","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 43 excessive sectors (22% of the total 199 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.73"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'261",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.31",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.06",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.44",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.02",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.57",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'811.08",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.46",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.72",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.15",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.72",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.5%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.54",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.31",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.31",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.43",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","51.50",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.46",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.8 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.906"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.232"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.72",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.28",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","11.72",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.62",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 11.72 active warps per scheduler, but only an average of 0.62 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.28"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.43",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.93",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 14.2 cycles being stalled waiting to be selected to fetch an instruction or waiting on an instruction cache miss. A high number of warps not having an instruction fetched is typical for very short kernels with less than one full wave of work in the grid. Excessively jumping across large blocks of assembly code can also lead to more warps stalled for this reason, if this causes misses in the instruction cache. See also the related Branch Resolving state. This stall type represents about 36.1% of the total average of 39.4 cycles between issuing two instructions.","global","36.08"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","489.20",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'298",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","520.20",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'667",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 18 fused and 14 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.001556"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.14",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.41",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.86"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","9",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","676'864",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'811.08",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","999'332",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","477.45",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","884'064",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'811.08",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","999'332",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'750.38",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","3'997'328",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.49% above the average, while the minimum instance value is 11.49% below the average.","global","16.14"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.91% above the average, while the minimum instance value is 14.70% below the average.","global","15.24"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.49% above the average, while the minimum instance value is 11.49% below the average.","global","16.14"
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'534",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.01",
"44","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 15 excessive sectors (22% of the total 67 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.161"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'982",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.17",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.05",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.89",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.96",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.32",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'815.25",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.16",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.62",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.14",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.62",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.09",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.17",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.17",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.33",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","50.92",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.16",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 20.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.565"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.131"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.99",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.01",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.85",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.85 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.01"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.85",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.91",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 14.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 44.3% of the total average of 32.9 cycles between issuing two instructions.","global","44.29"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.1 cycles being stalled waiting for the MIO (memory input/output) instruction queue to be not full. This stall reason is high in cases of extreme utilization of the MIO pipelines, which include special math instructions, dynamic branches, as well as shared memory instructions. When caused by shared memory accesses, trying to use fewer but wider loads can reduce pipeline pressure. This stall type represents about 30.8% of the total average of 32.9 cycles between issuing two instructions.","global","30.79"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","488.96",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'173",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","519.56",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'328",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9 fused and 7 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.000776"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.53",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.66",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.47"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","8",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","738'304",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'815.25",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'031'098",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","546.81",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","963'072",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'815.25",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'031'098",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'732.39",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'124'392",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.95% above the average, while the minimum instance value is 12.24% below the average.","global","15.79"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.97% above the average, while the minimum instance value is 13.13% below the average.","global","15.07"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.95% above the average, while the minimum instance value is 12.24% below the average.","global","15.79"
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'519",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.00",
"45","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (13% of the total 30 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","0.7268"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'603",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.11",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.05",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.66",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.24",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.33",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'787.30",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.03",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.09",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.71",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.71",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.8%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.17",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.11",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.11",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.33",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","64.70",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.03",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 20.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.542"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.084"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.95",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.05",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.38",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.38 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.05"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.52",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.40",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 41.6% of the total average of 33.5 cycles between issuing two instructions.","global","41.58"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","488.96",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'173",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","530.98",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","280'358",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9 fused and 7 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.0007881"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.46",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.26",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.51"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","8",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","706'816",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'787.30",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'046'546",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","548.92",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","921'600",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'787.30",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'046'546",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'715.61",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'186'184",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.85% above the average, while the minimum instance value is 13.44% below the average.","global","15.07"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.78% above the average, while the minimum instance value is 17.93% below the average.","global","14.88"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.85% above the average, while the minimum instance value is 13.44% below the average.","global","15.07"
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'519",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.00",
"46","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (13% of the total 30 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","0.7624"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'463",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.15",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.06",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.57",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.56",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.34",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'757.81",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.12",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.11",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.61",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.61",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (23.2%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","2.53",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.15",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.15",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","13.43",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","51.50",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.12",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.8 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.838"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.116"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.66",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.34",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.04",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.64",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.04 active warps per scheduler, but only an average of 0.64 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.34"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.86",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.03",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.99",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","489.20",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'298",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","520.56",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","274'856",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 18 fused and 14 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.001603"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.92",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.91",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.07"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","9.17",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","694'784",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'757.81",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'036'740",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","657.83",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","906'336",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'757.81",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'036'740",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'755.24",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'146'960",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.28% above the average, while the minimum instance value is 13.30% below the average.","global","15.28"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.89% above the average, while the minimum instance value is 21.61% below the average.","global","15.17"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.28% above the average, while the minimum instance value is 13.30% below the average.","global","15.28"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 72.65% above the average, while the minimum instance value is 78.11% below the average.","global","5.062"
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'534",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.01",
"47","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 15 excessive sectors (22% of the total 67 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.56"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'972",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.19",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.08",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.89",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","14.94",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.34",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'189.77",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.18",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.90",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","23.81",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.95",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","23.81",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","81.37"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","3.13",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.19",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.19",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","15.58",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","67.83",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.18",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.831"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.146"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.38",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.62",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.20",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.70",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.20 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","69.62"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.57",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.69",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.95",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.95",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","490.27",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","258'864",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","521.35",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","275'273",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 62 fused and 44 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.004093"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","70.51",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","45.13",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (70.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","28.37"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","12",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","737'280",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'189.77",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'029'740",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","913.11",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","961'056",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'189.77",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'029'740",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'715.82",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'118'960",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.60% above the average, while the minimum instance value is 11.04% below the average.","global","17.57"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.35% above the average, while the minimum instance value is 15.90% below the average.","global","15.03"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.60% above the average, while the minimum instance value is 11.04% below the average.","global","17.57"
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'602",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.18",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.02",
"48","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 43 excessive sectors (22% of the total 199 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","1.971"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","8'812",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","3.95",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.11",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.79",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.96",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.67",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'832.27",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.61",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.07",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.24",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.59",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.14",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.59",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.3%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","4.42",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","3.95",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.94",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.18",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","65.15",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.61",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.6 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.652"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","2.963"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.87",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.13",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.62",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.62 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.13"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.08",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.15",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.89",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.89",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","492.18",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","259'871",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","523.93",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","276'637",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 141 fused and 97 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 20% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.01083"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 35.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","63.89",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.89",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (63.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","35.09"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","16.67",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","724'992",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'832.27",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'099'342",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","1'456.64",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","944'544",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'832.27",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'099'342",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'949.96",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'397'368",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.15% above the average, while the minimum instance value is 11.69% below the average.","global","15.21"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.18% above the average, while the minimum instance value is 18.20% below the average.","global","15.73"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.15% above the average, while the minimum instance value is 11.69% below the average.","global","15.21"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 57.63% above the average, while the minimum instance value is 85.03% below the average.","global","8.532"
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","21'723",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","79.20",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.05",
"49","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 80 excessive sectors (19% of the total 418 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","2.833"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'032",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.16",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.22",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.95",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.75",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.42",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'881.02",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.97",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.06",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.11",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.12",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.11",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.9%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","8.95",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.16",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.14",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.49",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","58.53",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.97",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.6 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.876"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.12"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.93",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.07",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.01",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.65",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.01 active warps per scheduler, but only an average of 0.65 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.07"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.46",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.56",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.74",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.72",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","497.53",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","262'694",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","528.81",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","279'214",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 354 fused and 250 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.02709"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.97",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.58",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (65.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","34.67",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","742'912",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'881.02",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'059'570",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'103.56",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","967'488",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'881.02",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'059'570",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'767.01",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'238'280",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.57% above the average, while the minimum instance value is 16.80% below the average.","global","15.83"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 68.45% above the average, while the minimum instance value is 18.73% below the average.","global","15.07"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.57% above the average, while the minimum instance value is 16.80% below the average.","global","15.83"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 37.02% above the average, while the minimum instance value is 62.40% below the average.","global","7.727"
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'062",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.34",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.13",
"50","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 267 excessive sectors (23% of the total 1146 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","4.863"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'147",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.01",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.34",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.02",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.73",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.69",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1'809.61",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.55",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.11",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.24",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.60",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.60",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (22.9%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","13.62",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.01",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","3.98",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.88",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","38.27",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.55",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.3 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.838"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.006"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.48",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.52",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.89",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.63",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.89 active warps per scheduler, but only an average of 0.63 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.52"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.55",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.71",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.57",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.54",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","503.25",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","265'715",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","535.65",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","282'824",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 591 fused and 409 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 20% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.04619"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.40",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","39.94",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.61"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","53.33",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","750'336",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","1'809.61",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'116'052",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'420.31",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","979'776",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","1'809.61",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'116'052",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'817.23",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'464'208",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.67% above the average, while the minimum instance value is 16.45% below the average.","global","14.48"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 69.94% above the average, while the minimum instance value is 21.69% below the average.","global","15.03"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.67% above the average, while the minimum instance value is 16.45% below the average.","global","14.48"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 37.43% above the average, while the minimum instance value is 68.27% below the average.","global","8.876"
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.08",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","22'425",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","79.01",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.21",
"51","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 466 excessive sectors (25% of the total 1896 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","5.829"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'495",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.27",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.53",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.24",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.85",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","0.96",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'071.20",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.90",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.99",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.29",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.05",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.29",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (20.3%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","21.09",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.27",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.21",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.04",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","46.89",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.90",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.885"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.202"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.26",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.74",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.99",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.67",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.99 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.74"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.15",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.23",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","31.31",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.27",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","513.32",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","271'031",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","544.53",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","287'511",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 977 fused and 705 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.06918"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 34.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","64.71",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","41.41",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (64.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","34.26"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","85.67",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","780'032",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'071.20",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'078'696",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","2'899.78",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'015'872",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'071.20",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'078'696",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'860.80",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'314'784",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.09% above the average, while the minimum instance value is 20.92% below the average.","global","16.75"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.72% above the average, while the minimum instance value is 22.08% below the average.","global","15.19"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 66.09% above the average, while the minimum instance value is 20.92% below the average.","global","16.75"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 34.23% above the average, while the minimum instance value is 45.51% below the average.","global","9.38"
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","23'063",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.62",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.37",
"52","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 723 excessive sectors (22% of the total 3245 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","6.105"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'219",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.36",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.98",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.05",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","18.13",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","1.36",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'032.40",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.74",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.05",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.25",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.95",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.12",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.95",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (21.2%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","39.37",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.36",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.26",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.87",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","36.04",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.74",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 17.5 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","1.98"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.274"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.36",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.64",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.30",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.30 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.64"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.09",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.24",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","30.70",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.62",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","535.37",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","282'674",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","568.15",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","299'984",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 1890 fused and 1318 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.1324"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 39.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","59.93",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","38.36",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (59.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","39.11"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","155",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","756'736",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'032.40",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'114'238",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","3'597.57",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","985'344",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'032.40",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'114'238",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","1'935.20",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'456'952",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.41% above the average, while the minimum instance value is 21.08% below the average.","global","15.75"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 67.13% above the average, while the minimum instance value is 25.64% below the average.","global","15.39"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 65.41% above the average, while the minimum instance value is 21.08% below the average.","global","15.75"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 26.22% above the average, while the minimum instance value is 25.17% below the average.","global","9.19"
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","24'462",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","78.76",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0.67",
"53","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1434 excessive sectors (24% of the total 6042 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","8.319"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'777",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","4.76",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.66",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.43",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.56",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","2.59",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'317.66",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.87",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.99",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.27",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","26.31",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.05",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","26.31",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.58"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","66.35",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","4.76",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.57",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.37",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","33.28",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","8.87",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.065"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.573"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.49",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.51",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.02",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.59",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.02 active warps per scheduler, but only an average of 0.59 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.51"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.82",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.80",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","29.85",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.71",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","575.08",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","303'643",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","609.77",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","321'960",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 3429 fused and 2477 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.2172"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 41.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","57.81",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","37.00",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (57.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","41.28"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","277.83",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","803'840",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'317.66",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'127'706",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'094.78",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'046'304",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'317.66",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'127'706",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'218.07",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'510'824",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.60% above the average, while the minimum instance value is 29.80% below the average.","global","16.71"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 62.42% above the average, while the minimum instance value is 32.24% below the average.","global","16.21"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 61.60% above the average, while the minimum instance value is 29.80% below the average.","global","16.71"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 21.38% above the average, while the minimum instance value is 21.44% below the average.","global","8.031"
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","26'979",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.59",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","1.29",
"54","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 2359 excessive sectors (21% of the total 11200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","7.913"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'864",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","5.31",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","2.74",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.50",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.97",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","4.16",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2'734.35",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.02",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.29",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.68",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.68",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.41"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","109.32",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","5.31",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","4.98",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.93",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","29.84",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.02",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.324"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","3.981"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.26",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.74",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.71",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.71 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.74"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.17",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.15",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","28.65",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.43",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","636.77",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","336'217",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","674.86",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","356'326",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 5901 fused and 4235 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.315"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 42.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","56.50",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","36.16",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (56.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","42.6"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","462.33",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","811'008",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","2'734.35",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'153'770",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'638.74",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'052'544",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","2'734.35",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'153'770",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","2'569.73",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'615'080",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 55.64% above the average, while the minimum instance value is 39.62% below the average.","global","17.41"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 57.33% above the average, while the minimum instance value is 38.79% below the average.","global","16.86"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 55.64% above the average, while the minimum instance value is 39.62% below the average.","global","17.41"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 21.79% above the average, while the minimum instance value is 16.64% below the average.","global","9.218"
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.09",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","30'891",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.81",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","2.20",
"55","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 4178 excessive sectors (22% of the total 19254 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.181"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'855",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","6.30",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.20",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.50",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.57",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.58",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'089.52",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.65",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.33",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.60",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.60",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.13"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","167.01",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","6.30",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","5.78",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.89",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.93",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.65",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.73"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","4.722"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.18",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.82",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.37",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.56",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.37 active warps per scheduler, but only an average of 0.56 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.82"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.22",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.10",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","27.43",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.13",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","719.48",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","379'888",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","760.13",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","401'349",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 9146 fused and 6628 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.4358"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 47.6%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","51.54",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.98",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (51.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","47.64"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","706.33",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","806'912",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'089.52",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'137'950",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","4'954.05",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'052'928",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'089.52",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'137'950",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'018.45",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'551'800",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.87% above the average, while the minimum instance value is 45.46% below the average.","global","17.15"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 49.61% above the average, while the minimum instance value is 48.62% below the average.","global","17.37"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 47.87% above the average, while the minimum instance value is 45.46% below the average.","global","17.15"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 21.90% above the average, while the minimum instance value is 19.64% below the average.","global","9.891"
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","36'134",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.48",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","3.46",
"56","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 6339 excessive sectors (21% of the total 30043 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.53"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9'853",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","7.28",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","6.24",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.46",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.94",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","8.15",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3'914.61",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","9.78",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.86",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.37",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.56",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.90",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.56",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","85.19"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","250.34",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","7.28",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.48",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.80",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.13",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","9.78",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.187"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","5.459"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.86",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.14",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.90",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.49",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.90 active warps per scheduler, but only an average of 0.49 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.14"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.11",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","34.78",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","26.08",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.69",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","840.50",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","443'785",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","883.04",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","466'246",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 13883 fused and 10135 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.5254"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 50.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","48.33",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","30.93",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (48.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","50.9"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","1'053.50",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","810'496",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","3'914.61",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'202'536",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'141.82",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'051'584",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","3'914.61",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'202'536",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","3'701.32",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'810'144",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 34.99% above the average, while the minimum instance value is 52.72% below the average.","global","15.04"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 42.85% above the average, while the minimum instance value is 56.69% below the average.","global","17.41"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 34.99% above the average, while the minimum instance value is 52.72% below the average.","global","15.04"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.13% above the average, while the minimum instance value is 11.12% below the average.","global","10.39"
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","43'805",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.23",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","5.31",
"57","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 10035 excessive sectors (22% of the total 46393 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.15"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'016",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.81",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","7.83",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.23",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.94",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.39",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","4'816.25",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","11.34",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.83",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.43",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","21.81",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.87",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","21.81",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.39"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","313.10",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","8.81",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","7.83",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","16.97",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.33",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","10.46",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.1 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 85.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","3.875"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","6.605"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","23.29",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.23",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","76.71",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","8.85",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 8.85 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","76.71"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.00",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.05",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","24.83",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.35",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 12.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 31.7% of the total average of 38.0 cycles between issuing two instructions.","global","31.72"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","996.62",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","526'217",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'050.43",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","554'626",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 19987 fused and 14663 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.6174"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 48.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","50.90",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","32.58",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (50.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","48.29"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","1'474.17",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","904'192",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","4'816.25",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'222'682",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'411.08",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'172'160",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","4'816.25",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'222'682",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","4'509.84",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'890'728",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 28.21% above the average, while the minimum instance value is 55.92% below the average.","global","14.67"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 31.11% above the average, while the minimum instance value is 58.05% below the average.","global","15.14"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 28.21% above the average, while the minimum instance value is 55.92% below the average.","global","14.67"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.64% above the average, while the minimum instance value is 13.16% below the average.","global","7.817"
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.10",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","53'701",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.06",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","7.71",
"58","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 14308 excessive sectors (21% of the total 67058 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","9.456"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'753",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","11.29",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","10.88",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.07",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","17.76",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.98",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5'887.87",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","13.81",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.83",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.52",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","21.73",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.87",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","21.73",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","87.06"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","435.37",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","11.29",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","10.88",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.72",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.84",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.66",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","5.181"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","8.466"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","22.45",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","77.55",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.62",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.47",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.62 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","77.55"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.93",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","35.72",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","23.60",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","23.03",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 14.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 41.7% of the total average of 33.9 cycles between issuing two instructions.","global","41.74"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 23.6 threads being active per cycle. This is further reduced to 23.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","3.87"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'215.57",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","641'820",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'279.67",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","675'667",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 28580 fused and 20996 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.7231"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 52.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","47.05",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","30.11",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (47.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","52.2"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'004.50",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","884'736",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5'887.87",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'222'972",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'707.01",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'145'856",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5'887.87",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'222'972",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5'701.12",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'891'888",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.75% above the average, while the minimum instance value is 53.55% below the average.","global","7.469"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.09% above the average, while the minimum instance value is 55.50% below the average.","global","8.057"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.75% above the average, while the minimum instance value is 53.55% below the average.","global","7.469"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.17% above the average, while the minimum instance value is 10.92% below the average.","global","5.343"
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","67'580",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","77.01",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","11.05",
"59","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 20695 excessive sectors (22% of the total 96251 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.28"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'804",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","13.26",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","13.23",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.10",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","19.19",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","16.67",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6'445.48",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","15.71",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.87",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.60",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","22.73",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.91",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","22.73",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.87"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","530.20",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","13.26",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","13.23",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","17.89",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.62",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.46",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 84.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","6.19"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","9.944"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.07",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.93",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.67",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.58",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.67 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.93"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.56",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.43",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.98",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","22.36",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 16.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 42.8% of the total average of 38.6 cycles between issuing two instructions.","global","42.84"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 23.0 threads being active per cycle. This is further reduced to 22.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","4.731"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'397.58",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","737'924",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'465.23",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","773'644",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 35480 fused and 26388 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.8288"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 43.3%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","55.78",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","35.70",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (55.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","43.34"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'452.17",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","889'728",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6'445.48",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'231'174",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5'991.97",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'148'544",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6'445.48",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'231'174",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5'843.97",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4'924'696",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.49% above the average, while the minimum instance value is 8.15% below the average.","global","6.555"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.00% above the average, while the minimum instance value is 12.53% below the average.","global","6.263"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.49% above the average, while the minimum instance value is 8.15% below the average.","global","6.555"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.71% above the average, while the minimum instance value is 9.08% below the average.","global","7.365"
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","79'112",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.59",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","13.97",
"60","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 26438 excessive sectors (22% of the total 121871 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","10.86"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10'989",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","15.51",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","15.51",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.23",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","21.45",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","19.09",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6'807.04",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","16.96",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.65",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.75",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.75",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","86.09"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","620.71",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","14.70",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","15.51",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.73",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.46",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.66",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","6.988"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","11.02"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.18",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.82",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","9.93",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 9.93 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.82"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.53",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","38.19",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.38",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.72",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 18.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 50.0% of the total average of 36.5 cycles between issuing two instructions.","global","50.01"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.4 threads being active per cycle. This is further reduced to 21.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","5.448"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'611.66",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","850'956",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'684.68",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","889'510",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 43706 fused and 32672 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","0.971"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 36.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","62.79",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","40.18",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (62.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","36.22"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","2'922.50",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","904'448",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6'807.04",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'311'332",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6'727.47",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'166'880",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6'807.04",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'311'332",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","6'197.75",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'245'328",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.69% above the average, while the minimum instance value is 9.56% below the average.","global","6.636"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.67% above the average, while the minimum instance value is 9.84% below the average.","global","7.905"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.69% above the average, while the minimum instance value is 9.56% below the average.","global","6.636"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 19.05% above the average, while the minimum instance value is 13.12% below the average.","global","10.55"
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","92'678",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.41",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","17.34",
"61","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 32186 excessive sectors (21% of the total 150484 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.84"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'931",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","16.27",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","16.27",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.84",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.92",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","19.99",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6'833.50",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.27",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.05",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.70",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.46",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.10",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.46",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.89"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","653.19",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","15.91",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","16.27",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.83",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","25.96",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.04",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","7.323"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","11.93"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","27.51",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","72.49",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.60",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.68",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.60 active warps per scheduler, but only an average of 0.68 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","72.49"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.54",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.20",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","22.03",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.33",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 21.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 54.4% of the total average of 38.5 cycles between issuing two instructions.","global","54.4"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 22.0 threads being active per cycle. This is further reduced to 21.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.091"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'798.80",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","949'768",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1'876.44",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","990'760",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 50762 fused and 38236 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 21% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.131"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 33.2%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","65.78",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","42.10",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (65.8%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","33.18"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'334",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","983'552",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6'833.50",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'355'778",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","6'900.55",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'268'256",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6'833.50",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'355'778",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","6'822.05",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'423'112",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.75% above the average, while the minimum instance value is 10.17% below the average.","global","6.66"
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","104'534",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","76.14",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","20.37",
"62","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 37913 excessive sectors (21% of the total 176746 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.2"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'370",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.53",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.53",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.49",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","23.94",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.81",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'295.30",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.78",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.06",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","27.81",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.11",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","27.81",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.99"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","743.59",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.16",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.53",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.13",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.85",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.98",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.313"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.12"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","26.74",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","73.26",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","10.85",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.66",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 10.85 active warps per scheduler, but only an average of 0.66 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","73.26"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.57",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.41",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.84",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.13",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 22.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 54.9% of the total average of 40.6 cycles between issuing two instructions.","global","54.93"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.8 threads being active per cycle. This is further reduced to 21.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.381"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1'940.50",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'024'586",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'028.61",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'071'105",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 55980 fused and 42514 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.176"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 30.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","68.89",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","44.09",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (68.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","30.02"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'625",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","939'264",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'295.30",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'426'070",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'359.57",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'208'448",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'295.30",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'426'070",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'586.23",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'704'280",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 15.00% above the average, while the minimum instance value is 10.91% below the average.","global","8.768"
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","113'508",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.86",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","22.74",
"63","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 41934 excessive sectors (21% of the total 196723 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.46"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'449",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.68",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.68",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.55",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","24.67",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.49",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'609.58",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.47",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.08",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.79",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.10",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.12",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.10",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.94"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","788.03",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.97",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.68",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.33",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.83",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.96",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.556"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.48"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.22",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.78",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","11.87",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.78",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 11.87 active warps per scheduler, but only an average of 0.78 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.78"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.61",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.24",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.73",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.99",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 22.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 56.4% of the total average of 40.6 cycles between issuing two instructions.","global","56.37"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.7 threads being active per cycle. This is further reduced to 21.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.042"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'056.36",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'085'758",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'138.64",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'129'200",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 60196 fused and 46038 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.22"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 24.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","73.90",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","47.29",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (73.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","24.93"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","3'874.50",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","945'152",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'609.58",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'379'048",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'658.21",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'214'976",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'609.58",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'379'048",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'318.08",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'516'192",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.62% above the average, while the minimum instance value is 8.90% below the average.","global","7.635"
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","120'844",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.61",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","24.71",
"64","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 45634 excessive sectors (21% of the total 213648 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.92"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.50",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'539",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.58",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.58",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.65",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","25.43",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.32",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'779.36",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.22",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.11",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","28.89",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.16",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","28.89",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.64"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","820.18",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.68",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.58",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.42",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.69",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.18",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.831"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.01"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.99",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.01",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.45",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.80",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.45 active warps per scheduler, but only an average of 0.80 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.01"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","41.53",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.12",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.58",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.83",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 24.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.7% of the total average of 41.5 cycles between issuing two instructions.","global","58.66"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.6 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.407"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'164.35",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'142'775",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'247.36",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'186'608",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 64259 fused and 49253 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.277"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 232 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 21.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","33.33"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","77.37",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","49.52",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (77.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","21.4"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'083.83",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","952'320",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'779.36",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'398'158",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'757.50",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'225'440",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'779.36",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'398'158",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'494.86",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'592'632",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 19.03% above the average, while the minimum instance value is 8.55% below the average.","global","11.57"
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","127'685",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.53",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","26.47",
"65","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 48071 excessive sectors (21% of the total 227518 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.84"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'033",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.34",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.34",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.94",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.26",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","24.87",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'771.42",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.29",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.66",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.66",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.33"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","814.06",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.85",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.34",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.41",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.71",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.14",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.041"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.13"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.01",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.99",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.23",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.83",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.23 active warps per scheduler, but only an average of 0.83 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.99"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.07",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.72",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.56",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.81",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 57.1% of the total average of 44.1 cycles between issuing two instructions.","global","57.11"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.6 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.446"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'221.54",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'172'971",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'304.72",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'216'892",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 66209 fused and 51061 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.324"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","79.89",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","51.13",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (79.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","18.84"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'206",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","992'384",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'771.42",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'429'118",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'692.89",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'273'824",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'771.42",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'429'118",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'679.25",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'716'472",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.22% above the average, while the minimum instance value is 9.31% below the average.","global","7.666"
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","131'303",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.31",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","27.52",
"66","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 49832 excessive sectors (21% of the total 236107 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.24"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'727",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.25",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.25",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.74",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.70",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.12",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'812.30",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.66",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.16",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.83",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.23",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.21",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.23",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.18"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","848.86",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.13",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.25",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.61",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.05",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.21",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.479"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.35"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.58",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.42",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.29",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.85",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.29 active warps per scheduler, but only an average of 0.85 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.42"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.47",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.35",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.52",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.76",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.4% of the total average of 43.5 cycles between issuing two instructions.","global","59.37"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.606"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'263.64",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'195'203",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'361.32",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'246'779",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 67735 fused and 52345 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.349"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","81.04",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","51.87",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (81.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","17.67"
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'279.67",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","966'656",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'812.30",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'439'322",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'810.78",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'242'048",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'812.30",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'439'322",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'723.05",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'757'288",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","133'969",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.23",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","28.24",
"67","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 50969 excessive sectors (21% of the total 241799 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.73"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'674",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.62",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.62",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.71",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.61",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.19",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'946.42",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.58",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.15",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.83",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.84",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.84",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.33"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","865.23",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.24",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.62",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.56",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.23",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.25",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.514"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.43"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.70",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.30",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.19",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.85",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.19 active warps per scheduler, but only an average of 0.85 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.3"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.98",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.52",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.52",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.76",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.3% of the total average of 43.0 cycles between issuing two instructions.","global","58.32"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.583"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'289.21",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'208'702",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'371.45",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'252'126",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 68590 fused and 53162 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.347"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.46",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.77",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.23"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'344.17",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","964'352",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'946.42",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'450'456",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'756.89",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'238'304",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'946.42",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'450'456",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'724.19",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'801'824",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.17% above the average, while the minimum instance value is 9.85% below the average.","global","6.115"
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","135'586",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.13",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","28.72",
"68","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 51731 excessive sectors (21% of the total 245683 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.66"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'847",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.33",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.33",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.81",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.45",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.00",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'028.89",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.52",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.14",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.79",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.63",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.63",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.47"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","854.26",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.31",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.33",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.53",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.09",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.54",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.438"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.74"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.30",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.70",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.79",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.78",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.79 active warps per scheduler, but only an average of 0.78 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","71.7"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","45.19",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.84",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.52",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.75",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 57.1% of the total average of 45.2 cycles between issuing two instructions.","global","57.1"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.211"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'295.15",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'211'837",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'378.77",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'255'990",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 68767 fused and 53363 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.338"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.42",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.75",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.27"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'342.50",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","977'408",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'028.89",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'530'414",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'825.70",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'254'720",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'028.89",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'530'414",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'404.60",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'121'656",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 16.68% above the average, while the minimum instance value is 9.49% below the average.","global","9.985"
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","135'961",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.08",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","28.84",
"69","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 51513 excessive sectors (21% of the total 246038 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.54"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.52",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'787",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.55",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.55",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.74",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.41",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.30",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'778.68",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.78",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.68",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.68",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.92"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","865.55",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.46",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.55",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.57",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.21",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.37",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 83.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.558"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.6"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.64",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.36",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.61",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.85",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.61 active warps per scheduler, but only an average of 0.85 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.36"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.41",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.00",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.52",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.75",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.7% of the total average of 44.4 cycles between issuing two instructions.","global","59.71"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.656"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'304.29",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'216'663",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'386.44",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'260'041",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 69069 fused and 53657 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.388"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.62",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.88",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.07"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'363.83",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","971'904",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'778.68",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'446'012",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'830.97",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'248'576",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'778.68",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'446'012",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'788.21",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'784'048",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.98% above the average, while the minimum instance value is 9.44% below the average.","global","6.008"
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","136'539",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.04",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.02",
"70","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 52075 excessive sectors (21% of the total 247741 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.66"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.50",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'845",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.48",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.48",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.84",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.33",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.18",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'824.69",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.27",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.78",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.63",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.63",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.97"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","856.52",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.09",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.48",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.64",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.15",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.33",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.9% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.493"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.57"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","28.09",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.28",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","71.91",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.28",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.78",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.28 active warps per scheduler, but only an average of 0.78 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","71.91"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.72",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.30",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.50",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.74",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.9% of the total average of 43.7 cycles between issuing two instructions.","global","60.95"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.136"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'312.80",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'221'156",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'396.45",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'265'326",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 69386 fused and 53912 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.387"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.94",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.08",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.74"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'371.83",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","976'896",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'824.69",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'560'486",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'857.48",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'254'528",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'824.69",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'560'486",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'529.83",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'241'944",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.98% above the average, while the minimum instance value is 8.88% below the average.","global","8.403"
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","137'078",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.04",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.16",
"71","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 51850 excessive sectors (21% of the total 248203 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.56"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'939",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.44",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.44",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.87",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.31",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.76",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'211.53",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.30",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.13",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.86",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.40",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.18",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.40",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.63"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","857.59",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.96",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.44",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.99",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.37",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.66",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.309"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.97"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.16",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.84",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.18",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.18 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.84"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","45.50",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.14",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.49",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.72",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.1% of the total average of 45.5 cycles between issuing two instructions.","global","58.1"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.861"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'329.90",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'230'187",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'413.95",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'274'564",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 70007 fused and 54433 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.334"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.38",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.01",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.28"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'395.17",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","984'192",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'211.53",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'428'674",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'889.61",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'264'032",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'211.53",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'428'674",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'747.66",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'714'696",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 8.46% above the average, while the minimum instance value is 8.06% below the average.","global","5.071"
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","138'161",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.01",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.45",
"72","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 52506 excessive sectors (21% of the total 250781 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.55"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.50",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'653",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.83",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.83",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.71",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.70",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.68",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'032.91",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.41",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.15",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.87",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.86",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.86",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.37"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","870.37",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","20.04",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.83",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","19.93",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.87",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.76",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.691"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","15.03"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.22",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.78",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.55",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.89",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.55 active warps per scheduler, but only an average of 0.89 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.78"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.40",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.92",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.49",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.73",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.9% of the total average of 43.4 cycles between issuing two instructions.","global","58.85"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.897"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'317.12",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'223'437",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'398.58",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'266'450",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 69521 fused and 54055 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.354"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.50",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.44",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","15.17"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'370",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","960'768",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'032.91",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'412'526",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'705.55",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'235'040",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'032.91",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'412'526",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'683.79",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'650'104",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.58% above the average, while the minimum instance value is 8.42% below the average.","global","6.335"
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","137'351",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.01",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.25",
"73","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 52646 excessive sectors (21% of the total 249192 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.65"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'225",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.87",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.87",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.06",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.75",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.80",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'817.81",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.83",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.84",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.05",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.05",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.78"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","835.17",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.51",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.87",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.46",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.27",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.31",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.286"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.63"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.17",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.83",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.42",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.83",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.42 active warps per scheduler, but only an average of 0.83 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.83"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.49",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.08",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.47",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.70",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.9% of the total average of 44.5 cycles between issuing two instructions.","global","59.91"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.708"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'343.84",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'237'548",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'427.68",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'281'813",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 70524 fused and 54852 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.412"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","83.88",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.68",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (83.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.79"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'384.67",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'008'384",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'817.81",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'467'860",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'726.35",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'293'600",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'817.81",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'467'860",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'047.21",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'871'440",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.61% above the average, while the minimum instance value is 10.40% below the average.","global","5.511"
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","139'044",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.68",
"74","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 53229 excessive sectors (21% of the total 252910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.07"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12'430",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.67",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.67",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.19",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.89",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.12",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'848.47",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.12",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.85",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.15",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.15",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.75"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","828.16",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.81",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.67",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.48",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.27",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.48",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.0% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.043"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.85"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.70",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.30",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.53",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.53 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.3"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.08",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.66",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.45",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.68",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.6% of the total average of 44.1 cycles between issuing two instructions.","global","60.64"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.5 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.824"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'359.83",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'245'989",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'444.44",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'290'666",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 71135 fused and 55323 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.418"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.66",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.18",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'416.83",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'025'792",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'848.47",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'458'712",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'775.67",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'317'600",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'848.47",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'458'712",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'963.55",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'834'848",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 9.49% above the average, while the minimum instance value is 8.59% below the average.","global","5.377"
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","140'057",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","29.94",
"75","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 53693 excessive sectors (21% of the total 255106 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.92"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'816",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.88",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.88",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.78",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.36",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.47",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'085.89",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.36",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.86",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.53",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.22",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.53",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.08"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","879.21",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","20.04",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.88",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.95",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","26.70",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.61",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.466"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","15.03"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.42",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.58",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.69",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.69 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.58"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.58",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.11",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.42",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.65",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.7% of the total average of 43.6 cycles between issuing two instructions.","global","60.69"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.929"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'384.80",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'259'173",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'468.35",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'303'289",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 72081 fused and 56063 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.395"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","85.62",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.80",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (85.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.02"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'451",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","976'640",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'085.89",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'457'152",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'950.33",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'253'088",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'085.89",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'457'152",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'854.80",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'828'608",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.61% above the average, while the minimum instance value is 9.79% below the average.","global","6.462"
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","141'639",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","30.34",
"76","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 54418 excessive sectors (21% of the total 258572 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","12.82"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'806",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","22.01",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","22.01",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.81",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.65",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","26.57",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'343.75",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.97",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.15",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.85",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","29.71",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.19",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","29.71",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.52"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","879.90",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.71",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","22.01",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.94",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.28",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.33",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.507"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.78"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.37",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.63",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.42",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.42 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.63"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.20",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.74",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.42",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.65",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.6% of the total average of 44.2 cycles between issuing two instructions.","global","59.61"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.794"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'395.45",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'264'798",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'479.14",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'308'986",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 72486 fused and 56378 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.36"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.40",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.30",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.23"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'472.83",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","975'488",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'343.75",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'489'498",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'047.51",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'252'512",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'343.75",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'489'498",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'164.42",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'957'992",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 18.81% above the average, while the minimum instance value is 9.52% below the average.","global","11.6"
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","142'314",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","30.51",
"77","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 55038 excessive sectors (21% of the total 260607 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.03"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'399",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","23.19",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","23.19",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.52",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.29",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","28.62",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","7'997.89",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","22.55",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.22",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.87",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.66",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.27",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.66",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.62"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","928.41",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","20.16",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","23.19",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.86",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.26",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.54",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.6% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","10.25"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","15.12"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.51",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.49",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.65",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.88",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.65 active warps per scheduler, but only an average of 0.88 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.49"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.49",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.37",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.37",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.60",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 57.0% of the total average of 46.5 cycles between issuing two instructions.","global","57.04"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","8.037"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'433.69",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'284'989",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'531.98",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'336'886",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 73943 fused and 57507 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.447"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.27",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.85",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.34"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'545.33",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","940'800",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","7'997.89",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'481'894",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'015.15",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'205'568",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","7'997.89",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'481'894",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'034.59",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5'927'576",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 16.12% above the average, while the minimum instance value is 9.35% below the average.","global","10.29"
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","144'737",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.12",
"78","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56048 excessive sectors (21% of the total 265575 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.47"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","11'551",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","23.08",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","23.08",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.62",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.22",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","28.36",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'068.19",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.94",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.85",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.41",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.26",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.41",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.67"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","923.70",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.72",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","23.08",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.03",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.16",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","14.23",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 81.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","10.13"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.79"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.00",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.00",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.45",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.83",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.45 active warps per scheduler, but only an average of 0.83 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","70"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.85",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.39",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.35",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.58",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 59.9% of the total average of 44.9 cycles between issuing two instructions.","global","59.9"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.4 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.833"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'449.90",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'293'548",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'534.17",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'338'043",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 74556 fused and 57988 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.446"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.39",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.93",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.22"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'580",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","952'320",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'068.19",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'524'472",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7'981.98",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'222'272",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'068.19",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'524'472",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'448.28",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6'097'888",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 14.06% above the average, while the minimum instance value is 8.97% below the average.","global","8.815"
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","145'764",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.38",
"79","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56226 excessive sectors (21% of the total 267440 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.18"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'029",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.15",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.15",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.25",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.96",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.20",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'204.40",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.97",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.73",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.08",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.08",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.85"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","768.89",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.07",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.15",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.23",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.21",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.31",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.36"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.8"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.92",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.08",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.00",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.00 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.08"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.85",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.33",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.34",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.56",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.6% of the total average of 43.8 cycles between issuing two instructions.","global","61.56"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.785"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'466.47",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'302'298",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'550.01",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'346'406",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 75186 fused and 58478 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.434"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.39",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.93",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.22"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'629.33",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'160'192",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'204.40",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'774'004",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","8'874.21",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'490'208",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'204.40",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'774'004",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","7'989.65",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'096'016",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.21% above the average, while the minimum instance value is 3.62% below the average.","global","16.61"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.20% above the average, while the minimum instance value is 4.60% below the average.","global","14.98"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.21% above the average, while the minimum instance value is 3.62% below the average.","global","16.61"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 17.66% above the average, while the minimum instance value is 14.32% below the average.","global","10.1"
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","146'814",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.64",
"80","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56412 excessive sectors (21% of the total 269416 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.97"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'070",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","17.86",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","17.86",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.92",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.87",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","21.41",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'278.48",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.01",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.20",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.74",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.93",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.93",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.94"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","718.01",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.13",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","17.86",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.43",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.58",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.34",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","7.69"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.85"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.87",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.13",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.85",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.93",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.85 active warps per scheduler, but only an average of 0.93 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.13"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.45",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.93",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.32",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.54",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 62.5% of the total average of 43.4 cycles between issuing two instructions.","global","62.52"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.809"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'475.94",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'307'298",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'560.63",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'352'011",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 75546 fused and 58758 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.428"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","87.31",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.88",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (87.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.3"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'637.17",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'245'952",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'278.48",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'777'596",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","9'230.14",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'602'336",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'278.48",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'777'596",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'034.66",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'110'384",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.05% above the average, while the minimum instance value is 3.29% below the average.","global","16.63"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.26% above the average, while the minimum instance value is 4.76% below the average.","global","15.07"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.05% above the average, while the minimum instance value is 3.29% below the average.","global","16.63"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.94% above the average, while the minimum instance value is 14.61% below the average.","global","7.708"
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","147'414",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","31.80",
"81","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 56354 excessive sectors (21% of the total 270173 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","11.53"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'586",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.98",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.98",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.60",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.18",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.36",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'375.43",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","17.92",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.69",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.22",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.22",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.83"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","762.03",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.18",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.98",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.05",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.77",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.64",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.429"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.13"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.88",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.12",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.21",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.88",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.21 active warps per scheduler, but only an average of 0.88 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.12"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.03",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.57",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.27",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.48",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 58.4% of the total average of 46.0 cycles between issuing two instructions.","global","58.43"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.45"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'529.92",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'335'798",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'614.70",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'380'563",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 77598 fused and 60354 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.45"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.59",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.42",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","12.04"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'762.67",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'204'224",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'375.43",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'925'628",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","10'962.08",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'551'648",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'375.43",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'925'628",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'467.45",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'702'512",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.11% above the average, while the minimum instance value is 6.08% below the average.","global","15.57"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 28.14% above the average, while the minimum instance value is 7.87% below the average.","global","16.34"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.11% above the average, while the minimum instance value is 6.08% below the average.","global","15.57"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.12% above the average, while the minimum instance value is 11.25% below the average.","global","6.862"
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","150'834",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75.00",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","32.66",
"82","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 57953 excessive sectors (21% of the total 277685 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.15"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'989",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.28",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.28",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.89",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.40",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.37",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'250.64",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.54",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.22",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.51",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.26",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.51",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.66"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","733.70",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.71",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.28",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.06",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.89",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.04",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.1"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.53"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.43",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.57",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.98",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.98 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.57"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","44.49",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.97",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.28",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.50",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 27.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.7% of the total average of 44.5 cycles between issuing two instructions.","global","60.72"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.3 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.665"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'515.72",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'328'298",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'599.63",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'372'607",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 77058 fused and 59934 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.462"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","86.70",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","55.49",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","11.92"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'723.17",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'240'064",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'250.64",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'851'032",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","10'530.05",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'596'480",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'250.64",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'851'032",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'272.42",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'404'128",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.29% above the average, while the minimum instance value is 5.20% below the average.","global","16.06"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.83% above the average, while the minimum instance value is 7.09% below the average.","global","15.83"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.29% above the average, while the minimum instance value is 5.20% below the average.","global","16.06"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.51% above the average, while the minimum instance value is 8.98% below the average.","global","6.656"
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","149'934",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","32.43",
"83","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 58148 excessive sectors (21% of the total 276200 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.33"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'349",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.11",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.57",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.56",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'819.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.43",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.71",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.46",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.22",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.46",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.26"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","740.63",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.67",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.09",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.38",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","11.97",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.159"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.51"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.63",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.37",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.93",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.93 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.37"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","47.21",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.75",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.19",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.40",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 55.8% of the total average of 47.2 cycles between issuing two instructions.","global","55.79"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.2 threads being active per cycle. This is further reduced to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.678"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'601.89",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'373'798",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'686.79",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'418'625",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 80334 fused and 62482 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.426"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.73",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.23",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.92"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'875.83",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'269'504",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'819.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'924'804",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'315.09",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'633'056",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'819.44",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'924'804",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'495.74",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'699'216",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.74% above the average, while the minimum instance value is 9.44% below the average.","global","14.96"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.49% above the average, while the minimum instance value is 9.61% below the average.","global","16.02"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.74% above the average, while the minimum instance value is 9.44% below the average.","global","14.96"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.48% above the average, while the minimum instance value is 10.24% below the average.","global","7.638"
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","155'394",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","33.81",
"84","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 60365 excessive sectors (21% of the total 287631 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","13.96"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","14'821",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.16",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.16",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.76",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.88",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.26",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'723.44",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.55",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.19",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.79",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.23",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.79",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.09"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","768.97",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.79",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.16",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.13",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.90",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.06",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.4% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.422"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.6"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.34",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.66",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","15.36",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.85",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 15.36 active warps per scheduler, but only an average of 0.85 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.66"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","49.03",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","50.60",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.19",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.40",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 54.0% of the total average of 49.0 cycles between issuing two instructions.","global","53.96"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.2 threads being active per cycle. This is further reduced to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.721"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'602.36",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'374'048",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'685.71",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'418'055",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 80352 fused and 62496 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.442"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.42",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.03",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.24"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4'886.17",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'223'808",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'723.44",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'911'570",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'255.95",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'574'208",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'723.44",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'911'570",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'569.69",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'646'280",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.35% above the average, while the minimum instance value is 9.47% below the average.","global","15.87"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.75% above the average, while the minimum instance value is 11.11% below the average.","global","15.24"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.35% above the average, while the minimum instance value is 9.47% below the average.","global","15.87"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 10.07% above the average, while the minimum instance value is 8.95% below the average.","global","6.916"
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","155'424",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","33.82",
"85","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 60866 excessive sectors (21% of the total 288259 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.49"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'351",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.02",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.02",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.14",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.30",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.08",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'883.42",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.90",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.73",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.18",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.25",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.18",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","83.95"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","759.87",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.15",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.02",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.44",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.65",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.29",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.1% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.325"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.86"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.22",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.78",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.61",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.86",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.61 active warps per scheduler, but only an average of 0.86 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.78"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.59",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.98",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.11",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.32",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.0% of the total average of 43.6 cycles between issuing two instructions.","global","60.02"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.1 threads being active per cycle. This is further reduced to 20.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.9"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'684.51",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'417'423",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'770.10",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'462'615",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 83475 fused and 64925 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.471"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.89",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","54.33",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","13.77"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'018.33",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'266'432",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'883.42",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'934'916",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'929.74",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'632'864",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'883.42",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'934'916",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'873.59",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'739'664",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.82% above the average, while the minimum instance value is 10.54% below the average.","global","15.65"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.92% above the average, while the minimum instance value is 15.02% below the average.","global","15.09"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 25.82% above the average, while the minimum instance value is 10.54% below the average.","global","15.65"
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","160'629",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","35.13",
"86","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 62970 excessive sectors (21% of the total 299002 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.77"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'557",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.83",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","18.83",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.24",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.22",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.15",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8'967.08",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.62",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.21",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.72",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.07",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.24",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.07",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.01"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","755.65",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.91",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","18.83",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.34",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.07",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.12",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.344"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","12.68"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.55",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.45",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.82",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.87",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.82 active warps per scheduler, but only an average of 0.87 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.45"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.80",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.17",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.09",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.30",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.1% of the total average of 43.8 cycles between issuing two instructions.","global","60.12"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.1 threads being active per cycle. This is further reduced to 20.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","6.807"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'702.03",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'426'673",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'786.37",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'471'202",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 84141 fused and 65443 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.469"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","84.24",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","53.92",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (84.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","14.42"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'037.67",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'284'352",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8'967.08",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'975'540",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'092.90",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'653'408",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8'967.08",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'975'540",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8'832.84",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'902'160",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.79% above the average, while the minimum instance value is 11.59% below the average.","global","16.05"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 27.04% above the average, while the minimum instance value is 12.70% below the average.","global","15.96"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 26.79% above the average, while the minimum instance value is 11.59% below the average.","global","16.05"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.31% above the average, while the minimum instance value is 5.68% below the average.","global","5.135"
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","161'739",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","35.41",
"87","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 63112 excessive sectors (21% of the total 301065 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.72"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'566",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.33",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.33",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.27",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.22",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.30",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'569.87",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.50",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.16",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.75",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.07",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.20",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.07",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.66"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","773.56",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.65",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.33",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.34",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.12",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.62",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.2% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.435"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.24"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.82",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.32",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.18",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.47",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.89",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.47 active warps per scheduler, but only an average of 0.89 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.18"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.35",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.91",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.02",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.22",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 60.0% of the total average of 42.3 cycles between issuing two instructions.","global","59.97"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.0 threads being active per cycle. This is further reduced to 20.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.178"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'775.19",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'465'298",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'878.01",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'519'590",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 86922 fused and 67606 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.422"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.67",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.91",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.7%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.02"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'173.17",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'284'864",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'569.87",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'948'298",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'086.60",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'657'344",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'569.87",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'948'298",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'045.78",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'793'192",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.76% above the average, while the minimum instance value is 14.70% below the average.","global","15.4"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 24.08% above the average, while the minimum instance value is 14.26% below the average.","global","14.76"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.76% above the average, while the minimum instance value is 14.70% below the average.","global","15.4"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.72% above the average, while the minimum instance value is 6.68% below the average.","global","7.506"
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","166'374",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","36.58",
"88","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 65870 excessive sectors (21% of the total 311233 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.82"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'146",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","19.84",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","19.84",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","9.98",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.63",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","23.95",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'439.23",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.34",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.75",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.34",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.21",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.34",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.42"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","796.10",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","17.61",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","19.84",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","20.24",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.31",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","12.59",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 82.3% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","8.64"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.21"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","31.41",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.31",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","68.59",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","13.34",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.83",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 13.34 active warps per scheduler, but only an average of 0.83 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","68.59"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.48",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.77",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","21.02",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.22",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.2% of the total average of 42.5 cycles between issuing two instructions.","global","61.18"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.0 threads being active per cycle. This is further reduced to 20.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.118"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2'779.45",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'467'548",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2'863.84",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'512'109",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 87084 fused and 67732 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.444"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","81.44",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.12",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (81.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","17.27"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'174.67",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'252'096",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'439.23",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'954'796",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","11'963.66",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'607'232",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'439.23",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'954'796",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","9'117.16",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'819'184",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.25% above the average, while the minimum instance value is 14.80% below the average.","global","14.18"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 23.97% above the average, while the minimum instance value is 15.05% below the average.","global","14.75"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 22.25% above the average, while the minimum instance value is 14.80% below the average.","global","14.18"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.75% above the average, while the minimum instance value is 7.66% below the average.","global","5.539"
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","166'644",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","36.65",
"89","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 64953 excessive sectors (21% of the total 310969 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","14.93"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'619",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.60",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.60",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.30",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","28.74",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.46",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","9'956.69",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","21.15",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","1.05",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.22",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.82",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","31.38",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.26",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","31.38",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.03"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","824.57",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.37",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.60",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.78",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","28.12",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.81",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 18.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 80.7% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.013"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","14.53"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","30.22",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.30",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","69.78",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.72",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.81",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.72 active warps per scheduler, but only an average of 0.81 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","69.78"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.08",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.26",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.80",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.99",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 62.2% of the total average of 42.1 cycles between issuing two instructions.","global","62.24"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.8 threads being active per cycle. This is further reduced to 20.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.94"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3'038.92",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'604'548",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3'123.95",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'649'443",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 96948 fused and 75404 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.524"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","82.01",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","52.48",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (82.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","16.69"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'531.50",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'288'832",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","9'956.69",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1'949'622",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'668",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'659'072",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","9'956.69",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1'949'622",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","10'336.98",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","7'798'488",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 18.15% above the average, while the minimum instance value is 19.33% below the average.","global","12.23"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.94% above the average, while the minimum instance value is 23.50% below the average.","global","9.759"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 18.15% above the average, while the minimum instance value is 19.33% below the average.","global","12.23"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.26% above the average, while the minimum instance value is 7.29% below the average.","global","9.722"
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","183'084",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","40.80",
"90","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 71753 excessive sectors (21% of the total 344784 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.25"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.51",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","15'389",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","20.95",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","20.95",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","10.14",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.88",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","25.95",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","10'302.47",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","20.06",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.","",""
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and  close to 1% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.18",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.78",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","30.43",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.22",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","30.43",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","84.51"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","840.35",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","18.38",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","20.95",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.94",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","27.67",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","13.10",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 17.9 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 80.5% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","9.191"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global stores.","global","13.78"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","29.20",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.29",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","70.80",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","12.30",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.79",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 12.30 active warps per scheduler, but only an average of 0.79 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","70.8"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.13",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.29",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","20.79",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","19.97",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 26.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 62.0% of the total average of 42.1 cycles between issuing two instructions.","global","62.05"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 20.8 threads being active per cycle. This is further reduced to 20.0 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","7.537"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3'050.28",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1'610'548",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3'134.85",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1'655'201",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 97380 fused and 75740 non-fused FP64 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP64 performance could be increased by up to 22% (relative to its current performance). Check the Source page to identify where this kernel executes FP64 instructions.","global","1.479"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","672",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferL1",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1'024",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","32",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","16.38",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","688'128",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","2.59",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","3",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","16",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","3",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","63",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","98.44",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","79.86",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","51.11",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (98.4%) and measured achieved occupancy (79.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","18.87"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","5'549.83",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","1'271'296",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","10'302.47",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","2'063'232",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","12'340.20",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1'635'456",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","10'302.47",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","2'063'232",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","10'734.55",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","8'252'928",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.57% above the average, while the minimum instance value is 22.10% below the average.","global","10.92"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.50% above the average, while the minimum instance value is 22.07% below the average.","global","11.33"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.57% above the average, while the minimum instance value is 22.10% below the average.","global","10.92"
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","183'804",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","75",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","40.98",
"91","184682","hpcg_bench","127.0.0.1","void multicolorGSSmoothCsrKernel_nPerRow<int, double, double, 672, 32, 32>(const T1 *, const T1 *, const T1 *, const T2 *, const T2 *, const T3 *, const T3 *, T3, const int *, int, int, T3 *)","1","7","(672, 1, 1)","(1024, 1, 1)","0","9.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 72630 excessive sectors (21% of the total 346759 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","15.17"
